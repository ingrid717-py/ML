{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388c8320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to Prepare Movie Review Data for Sentiment Analysis (Text Classification) by Jason Brownlee. Machine Learning Mastery\n",
    "# https://machinelearningmastery.com/prepare-movie-review-data-sentiment-analysis/?utm_source=drip&amp;utm_medium=email&amp;utm_campaign=DLFNLP+Mini-Course&amp;utm_content=Day+7%3A+Movie+Review+Sentiment+Analysis\n",
    "\n",
    "# Deep Convolutional Neural Network for Sentiment Analysis (Text Classification) by Jason Brownlee. Machine Learning Mastery\n",
    "# https://machinelearningmastery.com/develop-word-embedding-model-predicting-movie-review-sentiment/\n",
    "\n",
    "# Load Text Data\n",
    "# This loads the document as ASCII and preserves any white space, like new lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaa97545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing that it works...load one file\n",
    "filename = 'dataset//txt_sentoken//neg//cv000_29416.txt'\n",
    "# open the file as read only\n",
    "file = open(filename, 'r')\n",
    "# read all text\n",
    "text = file.read()\n",
    "# close the file\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ae713d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have two directories each with 1,000 documents each. We can process each directory in turn by first getting a \n",
    "# list of files in the directory using the listdir() function, then loading each file in turn.\n",
    "# For example, we can load each document in the negative directory using the load_doc() function to do the actual loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "462ac9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60434862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded cv000_29416.txt\n",
      "Loaded cv001_19502.txt\n",
      "Loaded cv002_17424.txt\n",
      "Loaded cv003_12683.txt\n",
      "Loaded cv004_12641.txt\n",
      "Loaded cv005_29357.txt\n",
      "Loaded cv006_17022.txt\n",
      "Loaded cv007_4992.txt\n",
      "Loaded cv008_29326.txt\n",
      "Loaded cv009_29417.txt\n",
      "Loaded cv010_29063.txt\n",
      "Loaded cv011_13044.txt\n",
      "Loaded cv012_29411.txt\n",
      "Loaded cv013_10494.txt\n",
      "Loaded cv014_15600.txt\n",
      "Loaded cv015_29356.txt\n",
      "Loaded cv016_4348.txt\n",
      "Loaded cv017_23487.txt\n",
      "Loaded cv018_21672.txt\n",
      "Loaded cv019_16117.txt\n",
      "Loaded cv020_9234.txt\n",
      "Loaded cv021_17313.txt\n",
      "Loaded cv022_14227.txt\n",
      "Loaded cv023_13847.txt\n",
      "Loaded cv024_7033.txt\n",
      "Loaded cv025_29825.txt\n",
      "Loaded cv026_29229.txt\n",
      "Loaded cv027_26270.txt\n",
      "Loaded cv028_26964.txt\n",
      "Loaded cv029_19943.txt\n",
      "Loaded cv030_22893.txt\n",
      "Loaded cv031_19540.txt\n",
      "Loaded cv032_23718.txt\n",
      "Loaded cv033_25680.txt\n",
      "Loaded cv034_29446.txt\n",
      "Loaded cv035_3343.txt\n",
      "Loaded cv036_18385.txt\n",
      "Loaded cv037_19798.txt\n",
      "Loaded cv038_9781.txt\n",
      "Loaded cv039_5963.txt\n",
      "Loaded cv040_8829.txt\n",
      "Loaded cv041_22364.txt\n",
      "Loaded cv042_11927.txt\n",
      "Loaded cv043_16808.txt\n",
      "Loaded cv044_18429.txt\n",
      "Loaded cv045_25077.txt\n",
      "Loaded cv046_10613.txt\n",
      "Loaded cv047_18725.txt\n",
      "Loaded cv048_18380.txt\n",
      "Loaded cv049_21917.txt\n",
      "Loaded cv050_12128.txt\n",
      "Loaded cv051_10751.txt\n",
      "Loaded cv052_29318.txt\n",
      "Loaded cv053_23117.txt\n",
      "Loaded cv054_4101.txt\n",
      "Loaded cv055_8926.txt\n",
      "Loaded cv056_14663.txt\n",
      "Loaded cv057_7962.txt\n",
      "Loaded cv058_8469.txt\n",
      "Loaded cv059_28723.txt\n",
      "Loaded cv060_11754.txt\n",
      "Loaded cv061_9321.txt\n",
      "Loaded cv062_24556.txt\n",
      "Loaded cv063_28852.txt\n",
      "Loaded cv064_25842.txt\n",
      "Loaded cv065_16909.txt\n",
      "Loaded cv066_11668.txt\n",
      "Loaded cv067_21192.txt\n",
      "Loaded cv068_14810.txt\n",
      "Loaded cv069_11613.txt\n",
      "Loaded cv070_13249.txt\n",
      "Loaded cv071_12969.txt\n",
      "Loaded cv072_5928.txt\n",
      "Loaded cv073_23039.txt\n",
      "Loaded cv074_7188.txt\n",
      "Loaded cv075_6250.txt\n",
      "Loaded cv076_26009.txt\n",
      "Loaded cv077_23172.txt\n",
      "Loaded cv078_16506.txt\n",
      "Loaded cv079_12766.txt\n",
      "Loaded cv080_14899.txt\n",
      "Loaded cv081_18241.txt\n",
      "Loaded cv082_11979.txt\n",
      "Loaded cv083_25491.txt\n",
      "Loaded cv084_15183.txt\n",
      "Loaded cv085_15286.txt\n",
      "Loaded cv086_19488.txt\n",
      "Loaded cv087_2145.txt\n",
      "Loaded cv088_25274.txt\n",
      "Loaded cv089_12222.txt\n",
      "Loaded cv090_0049.txt\n",
      "Loaded cv091_7899.txt\n",
      "Loaded cv092_27987.txt\n",
      "Loaded cv093_15606.txt\n",
      "Loaded cv094_27868.txt\n",
      "Loaded cv095_28730.txt\n",
      "Loaded cv096_12262.txt\n",
      "Loaded cv097_26081.txt\n",
      "Loaded cv098_17021.txt\n",
      "Loaded cv099_11189.txt\n",
      "Loaded cv100_12406.txt\n",
      "Loaded cv101_10537.txt\n",
      "Loaded cv102_8306.txt\n",
      "Loaded cv103_11943.txt\n",
      "Loaded cv104_19176.txt\n",
      "Loaded cv105_19135.txt\n",
      "Loaded cv106_18379.txt\n",
      "Loaded cv107_25639.txt\n",
      "Loaded cv108_17064.txt\n",
      "Loaded cv109_22599.txt\n",
      "Loaded cv110_27832.txt\n",
      "Loaded cv111_12253.txt\n",
      "Loaded cv112_12178.txt\n",
      "Loaded cv113_24354.txt\n",
      "Loaded cv114_19501.txt\n",
      "Loaded cv115_26443.txt\n",
      "Loaded cv116_28734.txt\n",
      "Loaded cv117_25625.txt\n",
      "Loaded cv118_28837.txt\n",
      "Loaded cv119_9909.txt\n",
      "Loaded cv120_3793.txt\n",
      "Loaded cv121_18621.txt\n",
      "Loaded cv122_7891.txt\n",
      "Loaded cv123_12165.txt\n",
      "Loaded cv124_3903.txt\n",
      "Loaded cv125_9636.txt\n",
      "Loaded cv126_28821.txt\n",
      "Loaded cv127_16451.txt\n",
      "Loaded cv128_29444.txt\n",
      "Loaded cv129_18373.txt\n",
      "Loaded cv130_18521.txt\n",
      "Loaded cv131_11568.txt\n",
      "Loaded cv132_5423.txt\n",
      "Loaded cv133_18065.txt\n",
      "Loaded cv134_23300.txt\n",
      "Loaded cv135_12506.txt\n",
      "Loaded cv136_12384.txt\n",
      "Loaded cv137_17020.txt\n",
      "Loaded cv138_13903.txt\n",
      "Loaded cv139_14236.txt\n",
      "Loaded cv140_7963.txt\n",
      "Loaded cv141_17179.txt\n",
      "Loaded cv142_23657.txt\n",
      "Loaded cv143_21158.txt\n",
      "Loaded cv144_5010.txt\n",
      "Loaded cv145_12239.txt\n",
      "Loaded cv146_19587.txt\n",
      "Loaded cv147_22625.txt\n",
      "Loaded cv148_18084.txt\n",
      "Loaded cv149_17084.txt\n",
      "Loaded cv150_14279.txt\n",
      "Loaded cv151_17231.txt\n",
      "Loaded cv152_9052.txt\n",
      "Loaded cv153_11607.txt\n",
      "Loaded cv154_9562.txt\n",
      "Loaded cv155_7845.txt\n",
      "Loaded cv156_11119.txt\n",
      "Loaded cv157_29302.txt\n",
      "Loaded cv158_10914.txt\n",
      "Loaded cv159_29374.txt\n",
      "Loaded cv160_10848.txt\n",
      "Loaded cv161_12224.txt\n",
      "Loaded cv162_10977.txt\n",
      "Loaded cv163_10110.txt\n",
      "Loaded cv164_23451.txt\n",
      "Loaded cv165_2389.txt\n",
      "Loaded cv166_11959.txt\n",
      "Loaded cv167_18094.txt\n",
      "Loaded cv168_7435.txt\n",
      "Loaded cv169_24973.txt\n",
      "Loaded cv170_29808.txt\n",
      "Loaded cv171_15164.txt\n",
      "Loaded cv172_12037.txt\n",
      "Loaded cv173_4295.txt\n",
      "Loaded cv174_9735.txt\n",
      "Loaded cv175_7375.txt\n",
      "Loaded cv176_14196.txt\n",
      "Loaded cv177_10904.txt\n",
      "Loaded cv178_14380.txt\n",
      "Loaded cv179_9533.txt\n",
      "Loaded cv180_17823.txt\n",
      "Loaded cv181_16083.txt\n",
      "Loaded cv182_7791.txt\n",
      "Loaded cv183_19826.txt\n",
      "Loaded cv184_26935.txt\n",
      "Loaded cv185_28372.txt\n",
      "Loaded cv186_2396.txt\n",
      "Loaded cv187_14112.txt\n",
      "Loaded cv188_20687.txt\n",
      "Loaded cv189_24248.txt\n",
      "Loaded cv190_27176.txt\n",
      "Loaded cv191_29539.txt\n",
      "Loaded cv192_16079.txt\n",
      "Loaded cv193_5393.txt\n",
      "Loaded cv194_12855.txt\n",
      "Loaded cv195_16146.txt\n",
      "Loaded cv196_28898.txt\n",
      "Loaded cv197_29271.txt\n",
      "Loaded cv198_19313.txt\n",
      "Loaded cv199_9721.txt\n",
      "Loaded cv200_29006.txt\n",
      "Loaded cv201_7421.txt\n",
      "Loaded cv202_11382.txt\n",
      "Loaded cv203_19052.txt\n",
      "Loaded cv204_8930.txt\n",
      "Loaded cv205_9676.txt\n",
      "Loaded cv206_15893.txt\n",
      "Loaded cv207_29141.txt\n",
      "Loaded cv208_9475.txt\n",
      "Loaded cv209_28973.txt\n",
      "Loaded cv210_9557.txt\n",
      "Loaded cv211_9955.txt\n",
      "Loaded cv212_10054.txt\n",
      "Loaded cv213_20300.txt\n",
      "Loaded cv214_13285.txt\n",
      "Loaded cv215_23246.txt\n",
      "Loaded cv216_20165.txt\n",
      "Loaded cv217_28707.txt\n",
      "Loaded cv218_25651.txt\n",
      "Loaded cv219_19874.txt\n",
      "Loaded cv220_28906.txt\n",
      "Loaded cv221_27081.txt\n",
      "Loaded cv222_18720.txt\n",
      "Loaded cv223_28923.txt\n",
      "Loaded cv224_18875.txt\n",
      "Loaded cv225_29083.txt\n",
      "Loaded cv226_26692.txt\n",
      "Loaded cv227_25406.txt\n",
      "Loaded cv228_5644.txt\n",
      "Loaded cv229_15200.txt\n",
      "Loaded cv230_7913.txt\n",
      "Loaded cv231_11028.txt\n",
      "Loaded cv232_16768.txt\n",
      "Loaded cv233_17614.txt\n",
      "Loaded cv234_22123.txt\n",
      "Loaded cv235_10704.txt\n",
      "Loaded cv236_12427.txt\n",
      "Loaded cv237_20635.txt\n",
      "Loaded cv238_14285.txt\n",
      "Loaded cv239_29828.txt\n",
      "Loaded cv240_15948.txt\n",
      "Loaded cv241_24602.txt\n",
      "Loaded cv242_11354.txt\n",
      "Loaded cv243_22164.txt\n",
      "Loaded cv244_22935.txt\n",
      "Loaded cv245_8938.txt\n",
      "Loaded cv246_28668.txt\n",
      "Loaded cv247_14668.txt\n",
      "Loaded cv248_15672.txt\n",
      "Loaded cv249_12674.txt\n",
      "Loaded cv250_26462.txt\n",
      "Loaded cv251_23901.txt\n",
      "Loaded cv252_24974.txt\n",
      "Loaded cv253_10190.txt\n",
      "Loaded cv254_5870.txt\n",
      "Loaded cv255_15267.txt\n",
      "Loaded cv256_16529.txt\n",
      "Loaded cv257_11856.txt\n",
      "Loaded cv258_5627.txt\n",
      "Loaded cv259_11827.txt\n",
      "Loaded cv260_15652.txt\n",
      "Loaded cv261_11855.txt\n",
      "Loaded cv262_13812.txt\n",
      "Loaded cv263_20693.txt\n",
      "Loaded cv264_14108.txt\n",
      "Loaded cv265_11625.txt\n",
      "Loaded cv266_26644.txt\n",
      "Loaded cv267_16618.txt\n",
      "Loaded cv268_20288.txt\n",
      "Loaded cv269_23018.txt\n",
      "Loaded cv270_5873.txt\n",
      "Loaded cv271_15364.txt\n",
      "Loaded cv272_20313.txt\n",
      "Loaded cv273_28961.txt\n",
      "Loaded cv274_26379.txt\n",
      "Loaded cv275_28725.txt\n",
      "Loaded cv276_17126.txt\n",
      "Loaded cv277_20467.txt\n",
      "Loaded cv278_14533.txt\n",
      "Loaded cv279_19452.txt\n",
      "Loaded cv280_8651.txt\n",
      "Loaded cv281_24711.txt\n",
      "Loaded cv282_6833.txt\n",
      "Loaded cv283_11963.txt\n",
      "Loaded cv284_20530.txt\n",
      "Loaded cv285_18186.txt\n",
      "Loaded cv286_26156.txt\n",
      "Loaded cv287_17410.txt\n",
      "Loaded cv288_20212.txt\n",
      "Loaded cv289_6239.txt\n",
      "Loaded cv290_11981.txt\n",
      "Loaded cv291_26844.txt\n",
      "Loaded cv292_7804.txt\n",
      "Loaded cv293_29731.txt\n",
      "Loaded cv294_12695.txt\n",
      "Loaded cv295_17060.txt\n",
      "Loaded cv296_13146.txt\n",
      "Loaded cv297_10104.txt\n",
      "Loaded cv298_24487.txt\n",
      "Loaded cv299_17950.txt\n",
      "Loaded cv300_23302.txt\n",
      "Loaded cv301_13010.txt\n",
      "Loaded cv302_26481.txt\n",
      "Loaded cv303_27366.txt\n",
      "Loaded cv304_28489.txt\n",
      "Loaded cv305_9937.txt\n",
      "Loaded cv306_10859.txt\n",
      "Loaded cv307_26382.txt\n",
      "Loaded cv308_5079.txt\n",
      "Loaded cv309_23737.txt\n",
      "Loaded cv310_14568.txt\n",
      "Loaded cv311_17708.txt\n",
      "Loaded cv312_29308.txt\n",
      "Loaded cv313_19337.txt\n",
      "Loaded cv314_16095.txt\n",
      "Loaded cv315_12638.txt\n",
      "Loaded cv316_5972.txt\n",
      "Loaded cv317_25111.txt\n",
      "Loaded cv318_11146.txt\n",
      "Loaded cv319_16459.txt\n",
      "Loaded cv320_9693.txt\n",
      "Loaded cv321_14191.txt\n",
      "Loaded cv322_21820.txt\n",
      "Loaded cv323_29633.txt\n",
      "Loaded cv324_7502.txt\n",
      "Loaded cv325_18330.txt\n",
      "Loaded cv326_14777.txt\n",
      "Loaded cv327_21743.txt\n",
      "Loaded cv328_10908.txt\n",
      "Loaded cv329_29293.txt\n",
      "Loaded cv330_29675.txt\n",
      "Loaded cv331_8656.txt\n",
      "Loaded cv332_17997.txt\n",
      "Loaded cv333_9443.txt\n",
      "Loaded cv334_0074.txt\n",
      "Loaded cv335_16299.txt\n",
      "Loaded cv336_10363.txt\n",
      "Loaded cv337_29061.txt\n",
      "Loaded cv338_9183.txt\n",
      "Loaded cv339_22452.txt\n",
      "Loaded cv340_14776.txt\n",
      "Loaded cv341_25667.txt\n",
      "Loaded cv342_20917.txt\n",
      "Loaded cv343_10906.txt\n",
      "Loaded cv344_5376.txt\n",
      "Loaded cv345_9966.txt\n",
      "Loaded cv346_19198.txt\n",
      "Loaded cv347_14722.txt\n",
      "Loaded cv348_19207.txt\n",
      "Loaded cv349_15032.txt\n",
      "Loaded cv350_22139.txt\n",
      "Loaded cv351_17029.txt\n",
      "Loaded cv352_5414.txt\n",
      "Loaded cv353_19197.txt\n",
      "Loaded cv354_8573.txt\n",
      "Loaded cv355_18174.txt\n",
      "Loaded cv356_26170.txt\n",
      "Loaded cv357_14710.txt\n",
      "Loaded cv358_11557.txt\n",
      "Loaded cv359_6751.txt\n",
      "Loaded cv360_8927.txt\n",
      "Loaded cv361_28738.txt\n",
      "Loaded cv362_16985.txt\n",
      "Loaded cv363_29273.txt\n",
      "Loaded cv364_14254.txt\n",
      "Loaded cv365_12442.txt\n",
      "Loaded cv366_10709.txt\n",
      "Loaded cv367_24065.txt\n",
      "Loaded cv368_11090.txt\n",
      "Loaded cv369_14245.txt\n",
      "Loaded cv370_5338.txt\n",
      "Loaded cv371_8197.txt\n",
      "Loaded cv372_6654.txt\n",
      "Loaded cv373_21872.txt\n",
      "Loaded cv374_26455.txt\n",
      "Loaded cv375_9932.txt\n",
      "Loaded cv376_20883.txt\n",
      "Loaded cv377_8440.txt\n",
      "Loaded cv378_21982.txt\n",
      "Loaded cv379_23167.txt\n",
      "Loaded cv380_8164.txt\n",
      "Loaded cv381_21673.txt\n",
      "Loaded cv382_8393.txt\n",
      "Loaded cv383_14662.txt\n",
      "Loaded cv384_18536.txt\n",
      "Loaded cv385_29621.txt\n",
      "Loaded cv386_10229.txt\n",
      "Loaded cv387_12391.txt\n",
      "Loaded cv388_12810.txt\n",
      "Loaded cv389_9611.txt\n",
      "Loaded cv390_12187.txt\n",
      "Loaded cv391_11615.txt\n",
      "Loaded cv392_12238.txt\n",
      "Loaded cv393_29234.txt\n",
      "Loaded cv394_5311.txt\n",
      "Loaded cv395_11761.txt\n",
      "Loaded cv396_19127.txt\n",
      "Loaded cv397_28890.txt\n",
      "Loaded cv398_17047.txt\n",
      "Loaded cv399_28593.txt\n",
      "Loaded cv400_20631.txt\n",
      "Loaded cv401_13758.txt\n",
      "Loaded cv402_16097.txt\n",
      "Loaded cv403_6721.txt\n",
      "Loaded cv404_21805.txt\n",
      "Loaded cv405_21868.txt\n",
      "Loaded cv406_22199.txt\n",
      "Loaded cv407_23928.txt\n",
      "Loaded cv408_5367.txt\n",
      "Loaded cv409_29625.txt\n",
      "Loaded cv410_25624.txt\n",
      "Loaded cv411_16799.txt\n",
      "Loaded cv412_25254.txt\n",
      "Loaded cv413_7893.txt\n",
      "Loaded cv414_11161.txt\n",
      "Loaded cv415_23674.txt\n",
      "Loaded cv416_12048.txt\n",
      "Loaded cv417_14653.txt\n",
      "Loaded cv418_16562.txt\n",
      "Loaded cv419_14799.txt\n",
      "Loaded cv420_28631.txt\n",
      "Loaded cv421_9752.txt\n",
      "Loaded cv422_9632.txt\n",
      "Loaded cv423_12089.txt\n",
      "Loaded cv424_9268.txt\n",
      "Loaded cv425_8603.txt\n",
      "Loaded cv426_10976.txt\n",
      "Loaded cv427_11693.txt\n",
      "Loaded cv428_12202.txt\n",
      "Loaded cv429_7937.txt\n",
      "Loaded cv430_18662.txt\n",
      "Loaded cv431_7538.txt\n",
      "Loaded cv432_15873.txt\n",
      "Loaded cv433_10443.txt\n",
      "Loaded cv434_5641.txt\n",
      "Loaded cv435_24355.txt\n",
      "Loaded cv436_20564.txt\n",
      "Loaded cv437_24070.txt\n",
      "Loaded cv438_8500.txt\n",
      "Loaded cv439_17633.txt\n",
      "Loaded cv440_16891.txt\n",
      "Loaded cv441_15276.txt\n",
      "Loaded cv442_15499.txt\n",
      "Loaded cv443_22367.txt\n",
      "Loaded cv444_9975.txt\n",
      "Loaded cv445_26683.txt\n",
      "Loaded cv446_12209.txt\n",
      "Loaded cv447_27334.txt\n",
      "Loaded cv448_16409.txt\n",
      "Loaded cv449_9126.txt\n",
      "Loaded cv450_8319.txt\n",
      "Loaded cv451_11502.txt\n",
      "Loaded cv452_5179.txt\n",
      "Loaded cv453_10911.txt\n",
      "Loaded cv454_21961.txt\n",
      "Loaded cv455_28866.txt\n",
      "Loaded cv456_20370.txt\n",
      "Loaded cv457_19546.txt\n",
      "Loaded cv458_9000.txt\n",
      "Loaded cv459_21834.txt\n",
      "Loaded cv460_11723.txt\n",
      "Loaded cv461_21124.txt\n",
      "Loaded cv462_20788.txt\n",
      "Loaded cv463_10846.txt\n",
      "Loaded cv464_17076.txt\n",
      "Loaded cv465_23401.txt\n",
      "Loaded cv466_20092.txt\n",
      "Loaded cv467_26610.txt\n",
      "Loaded cv468_16844.txt\n",
      "Loaded cv469_21998.txt\n",
      "Loaded cv470_17444.txt\n",
      "Loaded cv471_18405.txt\n",
      "Loaded cv472_29140.txt\n",
      "Loaded cv473_7869.txt\n",
      "Loaded cv474_10682.txt\n",
      "Loaded cv475_22978.txt\n",
      "Loaded cv476_18402.txt\n",
      "Loaded cv477_23530.txt\n",
      "Loaded cv478_15921.txt\n",
      "Loaded cv479_5450.txt\n",
      "Loaded cv480_21195.txt\n",
      "Loaded cv481_7930.txt\n",
      "Loaded cv482_11233.txt\n",
      "Loaded cv483_18103.txt\n",
      "Loaded cv484_26169.txt\n",
      "Loaded cv485_26879.txt\n",
      "Loaded cv486_9788.txt\n",
      "Loaded cv487_11058.txt\n",
      "Loaded cv488_21453.txt\n",
      "Loaded cv489_19046.txt\n",
      "Loaded cv490_18986.txt\n",
      "Loaded cv491_12992.txt\n",
      "Loaded cv492_19370.txt\n",
      "Loaded cv493_14135.txt\n",
      "Loaded cv494_18689.txt\n",
      "Loaded cv495_16121.txt\n",
      "Loaded cv496_11185.txt\n",
      "Loaded cv497_27086.txt\n",
      "Loaded cv498_9288.txt\n",
      "Loaded cv499_11407.txt\n",
      "Loaded cv500_10722.txt\n",
      "Loaded cv501_12675.txt\n",
      "Loaded cv502_10970.txt\n",
      "Loaded cv503_11196.txt\n",
      "Loaded cv504_29120.txt\n",
      "Loaded cv505_12926.txt\n",
      "Loaded cv506_17521.txt\n",
      "Loaded cv507_9509.txt\n",
      "Loaded cv508_17742.txt\n",
      "Loaded cv509_17354.txt\n",
      "Loaded cv510_24758.txt\n",
      "Loaded cv511_10360.txt\n",
      "Loaded cv512_17618.txt\n",
      "Loaded cv513_7236.txt\n",
      "Loaded cv514_12173.txt\n",
      "Loaded cv515_18484.txt\n",
      "Loaded cv516_12117.txt\n",
      "Loaded cv517_20616.txt\n",
      "Loaded cv518_14798.txt\n",
      "Loaded cv519_16239.txt\n",
      "Loaded cv520_13297.txt\n",
      "Loaded cv521_1730.txt\n",
      "Loaded cv522_5418.txt\n",
      "Loaded cv523_18285.txt\n",
      "Loaded cv524_24885.txt\n",
      "Loaded cv525_17930.txt\n",
      "Loaded cv526_12868.txt\n",
      "Loaded cv527_10338.txt\n",
      "Loaded cv528_11669.txt\n",
      "Loaded cv529_10972.txt\n",
      "Loaded cv530_17949.txt\n",
      "Loaded cv531_26838.txt\n",
      "Loaded cv532_6495.txt\n",
      "Loaded cv533_9843.txt\n",
      "Loaded cv534_15683.txt\n",
      "Loaded cv535_21183.txt\n",
      "Loaded cv536_27221.txt\n",
      "Loaded cv537_13516.txt\n",
      "Loaded cv538_28485.txt\n",
      "Loaded cv539_21865.txt\n",
      "Loaded cv540_3092.txt\n",
      "Loaded cv541_28683.txt\n",
      "Loaded cv542_20359.txt\n",
      "Loaded cv543_5107.txt\n",
      "Loaded cv544_5301.txt\n",
      "Loaded cv545_12848.txt\n",
      "Loaded cv546_12723.txt\n",
      "Loaded cv547_18043.txt\n",
      "Loaded cv548_18944.txt\n",
      "Loaded cv549_22771.txt\n",
      "Loaded cv550_23226.txt\n",
      "Loaded cv551_11214.txt\n",
      "Loaded cv552_0150.txt\n",
      "Loaded cv553_26965.txt\n",
      "Loaded cv554_14678.txt\n",
      "Loaded cv555_25047.txt\n",
      "Loaded cv556_16563.txt\n",
      "Loaded cv557_12237.txt\n",
      "Loaded cv558_29376.txt\n",
      "Loaded cv559_0057.txt\n",
      "Loaded cv560_18608.txt\n",
      "Loaded cv561_9484.txt\n",
      "Loaded cv562_10847.txt\n",
      "Loaded cv563_18610.txt\n",
      "Loaded cv564_12011.txt\n",
      "Loaded cv565_29403.txt\n",
      "Loaded cv566_8967.txt\n",
      "Loaded cv567_29420.txt\n",
      "Loaded cv568_17065.txt\n",
      "Loaded cv569_26750.txt\n",
      "Loaded cv570_28960.txt\n",
      "Loaded cv571_29292.txt\n",
      "Loaded cv572_20053.txt\n",
      "Loaded cv573_29384.txt\n",
      "Loaded cv574_23191.txt\n",
      "Loaded cv575_22598.txt\n",
      "Loaded cv576_15688.txt\n",
      "Loaded cv577_28220.txt\n",
      "Loaded cv578_16825.txt\n",
      "Loaded cv579_12542.txt\n",
      "Loaded cv580_15681.txt\n",
      "Loaded cv581_20790.txt\n",
      "Loaded cv582_6678.txt\n",
      "Loaded cv583_29465.txt\n",
      "Loaded cv584_29549.txt\n",
      "Loaded cv585_23576.txt\n",
      "Loaded cv586_8048.txt\n",
      "Loaded cv587_20532.txt\n",
      "Loaded cv588_14467.txt\n",
      "Loaded cv589_12853.txt\n",
      "Loaded cv590_20712.txt\n",
      "Loaded cv591_24887.txt\n",
      "Loaded cv592_23391.txt\n",
      "Loaded cv593_11931.txt\n",
      "Loaded cv594_11945.txt\n",
      "Loaded cv595_26420.txt\n",
      "Loaded cv596_4367.txt\n",
      "Loaded cv597_26744.txt\n",
      "Loaded cv598_18184.txt\n",
      "Loaded cv599_22197.txt\n",
      "Loaded cv600_25043.txt\n",
      "Loaded cv601_24759.txt\n",
      "Loaded cv602_8830.txt\n",
      "Loaded cv603_18885.txt\n",
      "Loaded cv604_23339.txt\n",
      "Loaded cv605_12730.txt\n",
      "Loaded cv606_17672.txt\n",
      "Loaded cv607_8235.txt\n",
      "Loaded cv608_24647.txt\n",
      "Loaded cv609_25038.txt\n",
      "Loaded cv610_24153.txt\n",
      "Loaded cv611_2253.txt\n",
      "Loaded cv612_5396.txt\n",
      "Loaded cv613_23104.txt\n",
      "Loaded cv614_11320.txt\n",
      "Loaded cv615_15734.txt\n",
      "Loaded cv616_29187.txt\n",
      "Loaded cv617_9561.txt\n",
      "Loaded cv618_9469.txt\n",
      "Loaded cv619_13677.txt\n",
      "Loaded cv620_2556.txt\n",
      "Loaded cv621_15984.txt\n",
      "Loaded cv622_8583.txt\n",
      "Loaded cv623_16988.txt\n",
      "Loaded cv624_11601.txt\n",
      "Loaded cv625_13518.txt\n",
      "Loaded cv626_7907.txt\n",
      "Loaded cv627_12603.txt\n",
      "Loaded cv628_20758.txt\n",
      "Loaded cv629_16604.txt\n",
      "Loaded cv630_10152.txt\n",
      "Loaded cv631_4782.txt\n",
      "Loaded cv632_9704.txt\n",
      "Loaded cv633_29730.txt\n",
      "Loaded cv634_11989.txt\n",
      "Loaded cv635_0984.txt\n",
      "Loaded cv636_16954.txt\n",
      "Loaded cv637_13682.txt\n",
      "Loaded cv638_29394.txt\n",
      "Loaded cv639_10797.txt\n",
      "Loaded cv640_5380.txt\n",
      "Loaded cv641_13412.txt\n",
      "Loaded cv642_29788.txt\n",
      "Loaded cv643_29282.txt\n",
      "Loaded cv644_18551.txt\n",
      "Loaded cv645_17078.txt\n",
      "Loaded cv646_16817.txt\n",
      "Loaded cv647_15275.txt\n",
      "Loaded cv648_17277.txt\n",
      "Loaded cv649_13947.txt\n",
      "Loaded cv650_15974.txt\n",
      "Loaded cv651_11120.txt\n",
      "Loaded cv652_15653.txt\n",
      "Loaded cv653_2107.txt\n",
      "Loaded cv654_19345.txt\n",
      "Loaded cv655_12055.txt\n",
      "Loaded cv656_25395.txt\n",
      "Loaded cv657_25835.txt\n",
      "Loaded cv658_11186.txt\n",
      "Loaded cv659_21483.txt\n",
      "Loaded cv660_23140.txt\n",
      "Loaded cv661_25780.txt\n",
      "Loaded cv662_14791.txt\n",
      "Loaded cv663_14484.txt\n",
      "Loaded cv664_4264.txt\n",
      "Loaded cv665_29386.txt\n",
      "Loaded cv666_20301.txt\n",
      "Loaded cv667_19672.txt\n",
      "Loaded cv668_18848.txt\n",
      "Loaded cv669_24318.txt\n",
      "Loaded cv670_2666.txt\n",
      "Loaded cv671_5164.txt\n",
      "Loaded cv672_27988.txt\n",
      "Loaded cv673_25874.txt\n",
      "Loaded cv674_11593.txt\n",
      "Loaded cv675_22871.txt\n",
      "Loaded cv676_22202.txt\n",
      "Loaded cv677_18938.txt\n",
      "Loaded cv678_14887.txt\n",
      "Loaded cv679_28221.txt\n",
      "Loaded cv680_10533.txt\n",
      "Loaded cv681_9744.txt\n",
      "Loaded cv682_17947.txt\n",
      "Loaded cv683_13047.txt\n",
      "Loaded cv684_12727.txt\n",
      "Loaded cv685_5710.txt\n",
      "Loaded cv686_15553.txt\n",
      "Loaded cv687_22207.txt\n",
      "Loaded cv688_7884.txt\n",
      "Loaded cv689_13701.txt\n",
      "Loaded cv690_5425.txt\n",
      "Loaded cv691_5090.txt\n",
      "Loaded cv692_17026.txt\n",
      "Loaded cv693_19147.txt\n",
      "Loaded cv694_4526.txt\n",
      "Loaded cv695_22268.txt\n",
      "Loaded cv696_29619.txt\n",
      "Loaded cv697_12106.txt\n",
      "Loaded cv698_16930.txt\n",
      "Loaded cv699_7773.txt\n",
      "Loaded cv700_23163.txt\n",
      "Loaded cv701_15880.txt\n",
      "Loaded cv702_12371.txt\n",
      "Loaded cv703_17948.txt\n",
      "Loaded cv704_17622.txt\n",
      "Loaded cv705_11973.txt\n",
      "Loaded cv706_25883.txt\n",
      "Loaded cv707_11421.txt\n",
      "Loaded cv708_28539.txt\n",
      "Loaded cv709_11173.txt\n",
      "Loaded cv710_23745.txt\n",
      "Loaded cv711_12687.txt\n",
      "Loaded cv712_24217.txt\n",
      "Loaded cv713_29002.txt\n",
      "Loaded cv714_19704.txt\n",
      "Loaded cv715_19246.txt\n",
      "Loaded cv716_11153.txt\n",
      "Loaded cv717_17472.txt\n",
      "Loaded cv718_12227.txt\n",
      "Loaded cv719_5581.txt\n",
      "Loaded cv720_5383.txt\n",
      "Loaded cv721_28993.txt\n",
      "Loaded cv722_7571.txt\n",
      "Loaded cv723_9002.txt\n",
      "Loaded cv724_15265.txt\n",
      "Loaded cv725_10266.txt\n",
      "Loaded cv726_4365.txt\n",
      "Loaded cv727_5006.txt\n",
      "Loaded cv728_17931.txt\n",
      "Loaded cv729_10475.txt\n",
      "Loaded cv730_10729.txt\n",
      "Loaded cv731_3968.txt\n",
      "Loaded cv732_13092.txt\n",
      "Loaded cv733_9891.txt\n",
      "Loaded cv734_22821.txt\n",
      "Loaded cv735_20218.txt\n",
      "Loaded cv736_24947.txt\n",
      "Loaded cv737_28733.txt\n",
      "Loaded cv738_10287.txt\n",
      "Loaded cv739_12179.txt\n",
      "Loaded cv740_13643.txt\n",
      "Loaded cv741_12765.txt\n",
      "Loaded cv742_8279.txt\n",
      "Loaded cv743_17023.txt\n",
      "Loaded cv744_10091.txt\n",
      "Loaded cv745_14009.txt\n",
      "Loaded cv746_10471.txt\n",
      "Loaded cv747_18189.txt\n",
      "Loaded cv748_14044.txt\n",
      "Loaded cv749_18960.txt\n",
      "Loaded cv750_10606.txt\n",
      "Loaded cv751_17208.txt\n",
      "Loaded cv752_25330.txt\n",
      "Loaded cv753_11812.txt\n",
      "Loaded cv754_7709.txt\n",
      "Loaded cv755_24881.txt\n",
      "Loaded cv756_23676.txt\n",
      "Loaded cv757_10668.txt\n",
      "Loaded cv758_9740.txt\n",
      "Loaded cv759_15091.txt\n",
      "Loaded cv760_8977.txt\n",
      "Loaded cv761_13769.txt\n",
      "Loaded cv762_15604.txt\n",
      "Loaded cv763_16486.txt\n",
      "Loaded cv764_12701.txt\n",
      "Loaded cv765_20429.txt\n",
      "Loaded cv766_7983.txt\n",
      "Loaded cv767_15673.txt\n",
      "Loaded cv768_12709.txt\n",
      "Loaded cv769_8565.txt\n",
      "Loaded cv770_11061.txt\n",
      "Loaded cv771_28466.txt\n",
      "Loaded cv772_12971.txt\n",
      "Loaded cv773_20264.txt\n",
      "Loaded cv774_15488.txt\n",
      "Loaded cv775_17966.txt\n",
      "Loaded cv776_21934.txt\n",
      "Loaded cv777_10247.txt\n",
      "Loaded cv778_18629.txt\n",
      "Loaded cv779_18989.txt\n",
      "Loaded cv780_8467.txt\n",
      "Loaded cv781_5358.txt\n",
      "Loaded cv782_21078.txt\n",
      "Loaded cv783_14724.txt\n",
      "Loaded cv784_16077.txt\n",
      "Loaded cv785_23748.txt\n",
      "Loaded cv786_23608.txt\n",
      "Loaded cv787_15277.txt\n",
      "Loaded cv788_26409.txt\n",
      "Loaded cv789_12991.txt\n",
      "Loaded cv790_16202.txt\n",
      "Loaded cv791_17995.txt\n",
      "Loaded cv792_3257.txt\n",
      "Loaded cv793_15235.txt\n",
      "Loaded cv794_17353.txt\n",
      "Loaded cv795_10291.txt\n",
      "Loaded cv796_17243.txt\n",
      "Loaded cv797_7245.txt\n",
      "Loaded cv798_24779.txt\n",
      "Loaded cv799_19812.txt\n",
      "Loaded cv800_13494.txt\n",
      "Loaded cv801_26335.txt\n",
      "Loaded cv802_28381.txt\n",
      "Loaded cv803_8584.txt\n",
      "Loaded cv804_11763.txt\n",
      "Loaded cv805_21128.txt\n",
      "Loaded cv806_9405.txt\n",
      "Loaded cv807_23024.txt\n",
      "Loaded cv808_13773.txt\n",
      "Loaded cv809_5012.txt\n",
      "Loaded cv810_13660.txt\n",
      "Loaded cv811_22646.txt\n",
      "Loaded cv812_19051.txt\n",
      "Loaded cv813_6649.txt\n",
      "Loaded cv814_20316.txt\n",
      "Loaded cv815_23466.txt\n",
      "Loaded cv816_15257.txt\n",
      "Loaded cv817_3675.txt\n",
      "Loaded cv818_10698.txt\n",
      "Loaded cv819_9567.txt\n",
      "Loaded cv820_24157.txt\n",
      "Loaded cv821_29283.txt\n",
      "Loaded cv822_21545.txt\n",
      "Loaded cv823_17055.txt\n",
      "Loaded cv824_9335.txt\n",
      "Loaded cv825_5168.txt\n",
      "Loaded cv826_12761.txt\n",
      "Loaded cv827_19479.txt\n",
      "Loaded cv828_21392.txt\n",
      "Loaded cv829_21725.txt\n",
      "Loaded cv830_5778.txt\n",
      "Loaded cv831_16325.txt\n",
      "Loaded cv832_24713.txt\n",
      "Loaded cv833_11961.txt\n",
      "Loaded cv834_23192.txt\n",
      "Loaded cv835_20531.txt\n",
      "Loaded cv836_14311.txt\n",
      "Loaded cv837_27232.txt\n",
      "Loaded cv838_25886.txt\n",
      "Loaded cv839_22807.txt\n",
      "Loaded cv840_18033.txt\n",
      "Loaded cv841_3367.txt\n",
      "Loaded cv842_5702.txt\n",
      "Loaded cv843_17054.txt\n",
      "Loaded cv844_13890.txt\n",
      "Loaded cv845_15886.txt\n",
      "Loaded cv846_29359.txt\n",
      "Loaded cv847_20855.txt\n",
      "Loaded cv848_10061.txt\n",
      "Loaded cv849_17215.txt\n",
      "Loaded cv850_18185.txt\n",
      "Loaded cv851_21895.txt\n",
      "Loaded cv852_27512.txt\n",
      "Loaded cv853_29119.txt\n",
      "Loaded cv854_18955.txt\n",
      "Loaded cv855_22134.txt\n",
      "Loaded cv856_28882.txt\n",
      "Loaded cv857_17527.txt\n",
      "Loaded cv858_20266.txt\n",
      "Loaded cv859_15689.txt\n",
      "Loaded cv860_15520.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded cv861_12809.txt\n",
      "Loaded cv862_15924.txt\n",
      "Loaded cv863_7912.txt\n",
      "Loaded cv864_3087.txt\n",
      "Loaded cv865_28796.txt\n",
      "Loaded cv866_29447.txt\n",
      "Loaded cv867_18362.txt\n",
      "Loaded cv868_12799.txt\n",
      "Loaded cv869_24782.txt\n",
      "Loaded cv870_18090.txt\n",
      "Loaded cv871_25971.txt\n",
      "Loaded cv872_13710.txt\n",
      "Loaded cv873_19937.txt\n",
      "Loaded cv874_12182.txt\n",
      "Loaded cv875_5622.txt\n",
      "Loaded cv876_9633.txt\n",
      "Loaded cv877_29132.txt\n",
      "Loaded cv878_17204.txt\n",
      "Loaded cv879_16585.txt\n",
      "Loaded cv880_29629.txt\n",
      "Loaded cv881_14767.txt\n",
      "Loaded cv882_10042.txt\n",
      "Loaded cv883_27621.txt\n",
      "Loaded cv884_15230.txt\n",
      "Loaded cv885_13390.txt\n",
      "Loaded cv886_19210.txt\n",
      "Loaded cv887_5306.txt\n",
      "Loaded cv888_25678.txt\n",
      "Loaded cv889_22670.txt\n",
      "Loaded cv890_3515.txt\n",
      "Loaded cv891_6035.txt\n",
      "Loaded cv892_18788.txt\n",
      "Loaded cv893_26731.txt\n",
      "Loaded cv894_22140.txt\n",
      "Loaded cv895_22200.txt\n",
      "Loaded cv896_17819.txt\n",
      "Loaded cv897_11703.txt\n",
      "Loaded cv898_1576.txt\n",
      "Loaded cv899_17812.txt\n",
      "Loaded cv900_10800.txt\n",
      "Loaded cv901_11934.txt\n",
      "Loaded cv902_13217.txt\n",
      "Loaded cv903_18981.txt\n",
      "Loaded cv904_25663.txt\n",
      "Loaded cv905_28965.txt\n",
      "Loaded cv906_12332.txt\n",
      "Loaded cv907_3193.txt\n",
      "Loaded cv908_17779.txt\n",
      "Loaded cv909_9973.txt\n",
      "Loaded cv910_21930.txt\n",
      "Loaded cv911_21695.txt\n",
      "Loaded cv912_5562.txt\n",
      "Loaded cv913_29127.txt\n",
      "Loaded cv914_2856.txt\n",
      "Loaded cv915_9342.txt\n",
      "Loaded cv916_17034.txt\n",
      "Loaded cv917_29484.txt\n",
      "Loaded cv918_27080.txt\n",
      "Loaded cv919_18155.txt\n",
      "Loaded cv920_29423.txt\n",
      "Loaded cv921_13988.txt\n",
      "Loaded cv922_10185.txt\n",
      "Loaded cv923_11951.txt\n",
      "Loaded cv924_29397.txt\n",
      "Loaded cv925_9459.txt\n",
      "Loaded cv926_18471.txt\n",
      "Loaded cv927_11471.txt\n",
      "Loaded cv928_9478.txt\n",
      "Loaded cv929_1841.txt\n",
      "Loaded cv930_14949.txt\n",
      "Loaded cv931_18783.txt\n",
      "Loaded cv932_14854.txt\n",
      "Loaded cv933_24953.txt\n",
      "Loaded cv934_20426.txt\n",
      "Loaded cv935_24977.txt\n",
      "Loaded cv936_17473.txt\n",
      "Loaded cv937_9816.txt\n",
      "Loaded cv938_10706.txt\n",
      "Loaded cv939_11247.txt\n",
      "Loaded cv940_18935.txt\n",
      "Loaded cv941_10718.txt\n",
      "Loaded cv942_18509.txt\n",
      "Loaded cv943_23547.txt\n",
      "Loaded cv944_15042.txt\n",
      "Loaded cv945_13012.txt\n",
      "Loaded cv946_20084.txt\n",
      "Loaded cv947_11316.txt\n",
      "Loaded cv948_25870.txt\n",
      "Loaded cv949_21565.txt\n",
      "Loaded cv950_13478.txt\n",
      "Loaded cv951_11816.txt\n",
      "Loaded cv952_26375.txt\n",
      "Loaded cv953_7078.txt\n",
      "Loaded cv954_19932.txt\n",
      "Loaded cv955_26154.txt\n",
      "Loaded cv956_12547.txt\n",
      "Loaded cv957_9059.txt\n",
      "Loaded cv958_13020.txt\n",
      "Loaded cv959_16218.txt\n",
      "Loaded cv960_28877.txt\n",
      "Loaded cv961_5578.txt\n",
      "Loaded cv962_9813.txt\n",
      "Loaded cv963_7208.txt\n",
      "Loaded cv964_5794.txt\n",
      "Loaded cv965_26688.txt\n",
      "Loaded cv966_28671.txt\n",
      "Loaded cv967_5626.txt\n",
      "Loaded cv968_25413.txt\n",
      "Loaded cv969_14760.txt\n",
      "Loaded cv970_19532.txt\n",
      "Loaded cv971_11790.txt\n",
      "Loaded cv972_26837.txt\n",
      "Loaded cv973_10171.txt\n",
      "Loaded cv974_24303.txt\n",
      "Loaded cv975_11920.txt\n",
      "Loaded cv976_10724.txt\n",
      "Loaded cv977_4776.txt\n",
      "Loaded cv978_22192.txt\n",
      "Loaded cv979_2029.txt\n",
      "Loaded cv980_11851.txt\n",
      "Loaded cv981_16679.txt\n",
      "Loaded cv982_22209.txt\n",
      "Loaded cv983_24219.txt\n",
      "Loaded cv984_14006.txt\n",
      "Loaded cv985_5964.txt\n",
      "Loaded cv986_15092.txt\n",
      "Loaded cv987_7394.txt\n",
      "Loaded cv988_20168.txt\n",
      "Loaded cv989_17297.txt\n",
      "Loaded cv990_12443.txt\n",
      "Loaded cv991_19973.txt\n",
      "Loaded cv992_12806.txt\n",
      "Loaded cv993_29565.txt\n",
      "Loaded cv994_13229.txt\n",
      "Loaded cv995_23113.txt\n",
      "Loaded cv996_12447.txt\n",
      "Loaded cv997_5152.txt\n",
      "Loaded cv998_15691.txt\n",
      "Loaded cv999_14636.txt\n"
     ]
    }
   ],
   "source": [
    "# load all docs in a directory\n",
    "def process_docs(directory):\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # skip files that do not have the right extension\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load document\n",
    "        doc = load_doc(path)\n",
    "        print('Loaded %s' % filename)\n",
    " \n",
    "# specify directory to load\n",
    "directory = 'dataset//txt_sentoken//neg'\n",
    "process_docs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04707e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean Text Data\n",
    "# Now that we know how to load the movie review text data, let’s look at cleaning it.\n",
    "\n",
    "# Split into Tokens\n",
    "# First, let’s load one document \n",
    "# and look at the raw tokens split by white space. We will use the load_doc() \n",
    "# function developed in the previous section. We can use the split() function to split the loaded document \n",
    "# into tokens separated by white space.\n",
    "# Running the example gives a nice long list of raw tokens from the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8c558f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.', 'they', 'get', 'into', 'an', 'accident', '.', 'one', 'of', 'the', 'guys', 'dies', ',', 'but', 'his', 'girlfriend', 'continues', 'to', 'see', 'him', 'in', 'her', 'life', ',', 'and', 'has', 'nightmares', '.', \"what's\", 'the', 'deal', '?', 'watch', 'the', 'movie', 'and', '\"', 'sorta', '\"', 'find', 'out', '.', '.', '.', 'critique', ':', 'a', 'mind-fuck', 'movie', 'for', 'the', 'teen', 'generation', 'that', 'touches', 'on', 'a', 'very', 'cool', 'idea', ',', 'but', 'presents', 'it', 'in', 'a', 'very', 'bad', 'package', '.', 'which', 'is', 'what', 'makes', 'this', 'review', 'an', 'even', 'harder', 'one', 'to', 'write', ',', 'since', 'i', 'generally', 'applaud', 'films', 'which', 'attempt', 'to', 'break', 'the', 'mold', ',', 'mess', 'with', 'your', 'head', 'and', 'such', '(', 'lost', 'highway', '&', 'memento', ')', ',', 'but', 'there', 'are', 'good', 'and', 'bad', 'ways', 'of', 'making', 'all', 'types', 'of', 'films', ',', 'and', 'these', 'folks', 'just', \"didn't\", 'snag', 'this', 'one', 'correctly', '.', 'they', 'seem', 'to', 'have', 'taken', 'this', 'pretty', 'neat', 'concept', ',', 'but', 'executed', 'it', 'terribly', '.', 'so', 'what', 'are', 'the', 'problems', 'with', 'the', 'movie', '?', 'well', ',', 'its', 'main', 'problem', 'is', 'that', \"it's\", 'simply', 'too', 'jumbled', '.', 'it', 'starts', 'off', '\"', 'normal', '\"', 'but', 'then', 'downshifts', 'into', 'this', '\"', 'fantasy', '\"', 'world', 'in', 'which', 'you', ',', 'as', 'an', 'audience', 'member', ',', 'have', 'no', 'idea', \"what's\", 'going', 'on', '.', 'there', 'are', 'dreams', ',', 'there', 'are', 'characters', 'coming', 'back', 'from', 'the', 'dead', ',', 'there', 'are', 'others', 'who', 'look', 'like', 'the', 'dead', ',', 'there', 'are', 'strange', 'apparitions', ',', 'there', 'are', 'disappearances', ',', 'there', 'are', 'a', 'looooot', 'of', 'chase', 'scenes', ',', 'there', 'are', 'tons', 'of', 'weird', 'things', 'that', 'happen', ',', 'and', 'most', 'of', 'it', 'is', 'simply', 'not', 'explained', '.', 'now', 'i', 'personally', \"don't\", 'mind', 'trying', 'to', 'unravel', 'a', 'film', 'every', 'now', 'and', 'then', ',', 'but', 'when', 'all', 'it', 'does', 'is', 'give', 'me', 'the', 'same', 'clue', 'over', 'and', 'over', 'again', ',', 'i', 'get', 'kind', 'of', 'fed', 'up', 'after', 'a', 'while', ',', 'which', 'is', 'this', \"film's\", 'biggest', 'problem', '.', \"it's\", 'obviously', 'got', 'this', 'big', 'secret', 'to', 'hide', ',', 'but', 'it', 'seems', 'to', 'want', 'to', 'hide', 'it', 'completely', 'until', 'its', 'final', 'five', 'minutes', '.', 'and', 'do', 'they', 'make', 'things', 'entertaining', ',', 'thrilling', 'or', 'even', 'engaging', ',', 'in', 'the', 'meantime', '?', 'not', 'really', '.', 'the', 'sad', 'part', 'is', 'that', 'the', 'arrow', 'and', 'i', 'both', 'dig', 'on', 'flicks', 'like', 'this', ',', 'so', 'we', 'actually', 'figured', 'most', 'of', 'it', 'out', 'by', 'the', 'half-way', 'point', ',', 'so', 'all', 'of', 'the', 'strangeness', 'after', 'that', 'did', 'start', 'to', 'make', 'a', 'little', 'bit', 'of', 'sense', ',', 'but', 'it', 'still', \"didn't\", 'the', 'make', 'the', 'film', 'all', 'that', 'more', 'entertaining', '.', 'i', 'guess', 'the', 'bottom', 'line', 'with', 'movies', 'like', 'this', 'is', 'that', 'you', 'should', 'always', 'make', 'sure', 'that', 'the', 'audience', 'is', '\"', 'into', 'it', '\"', 'even', 'before', 'they', 'are', 'given', 'the', 'secret', 'password', 'to', 'enter', 'your', 'world', 'of', 'understanding', '.', 'i', 'mean', ',', 'showing', 'melissa', 'sagemiller', 'running', 'away', 'from', 'visions', 'for', 'about', '20', 'minutes', 'throughout', 'the', 'movie', 'is', 'just', 'plain', 'lazy', '!', '!', 'okay', ',', 'we', 'get', 'it', '.', '.', '.', 'there', 'are', 'people', 'chasing', 'her', 'and', 'we', \"don't\", 'know', 'who', 'they', 'are', '.', 'do', 'we', 'really', 'need', 'to', 'see', 'it', 'over', 'and', 'over', 'again', '?', 'how', 'about', 'giving', 'us', 'different', 'scenes', 'offering', 'further', 'insight', 'into', 'all', 'of', 'the', 'strangeness', 'going', 'down', 'in', 'the', 'movie', '?', 'apparently', ',', 'the', 'studio', 'took', 'this', 'film', 'away', 'from', 'its', 'director', 'and', 'chopped', 'it', 'up', 'themselves', ',', 'and', 'it', 'shows', '.', 'there', \"might've\", 'been', 'a', 'pretty', 'decent', 'teen', 'mind-fuck', 'movie', 'in', 'here', 'somewhere', ',', 'but', 'i', 'guess', '\"', 'the', 'suits', '\"', 'decided', 'that', 'turning', 'it', 'into', 'a', 'music', 'video', 'with', 'little', 'edge', ',', 'would', 'make', 'more', 'sense', '.', 'the', 'actors', 'are', 'pretty', 'good', 'for', 'the', 'most', 'part', ',', 'although', 'wes', 'bentley', 'just', 'seemed', 'to', 'be', 'playing', 'the', 'exact', 'same', 'character', 'that', 'he', 'did', 'in', 'american', 'beauty', ',', 'only', 'in', 'a', 'new', 'neighborhood', '.', 'but', 'my', 'biggest', 'kudos', 'go', 'out', 'to', 'sagemiller', ',', 'who', 'holds', 'her', 'own', 'throughout', 'the', 'entire', 'film', ',', 'and', 'actually', 'has', 'you', 'feeling', 'her', \"character's\", 'unraveling', '.', 'overall', ',', 'the', 'film', \"doesn't\", 'stick', 'because', 'it', \"doesn't\", 'entertain', ',', \"it's\", 'confusing', ',', 'it', 'rarely', 'excites', 'and', 'it', 'feels', 'pretty', 'redundant', 'for', 'most', 'of', 'its', 'runtime', ',', 'despite', 'a', 'pretty', 'cool', 'ending', 'and', 'explanation', 'to', 'all', 'of', 'the', 'craziness', 'that', 'came', 'before', 'it', '.', 'oh', ',', 'and', 'by', 'the', 'way', ',', 'this', 'is', 'not', 'a', 'horror', 'or', 'teen', 'slasher', 'flick', '.', '.', '.', \"it's\", 'just', 'packaged', 'to', 'look', 'that', 'way', 'because', 'someone', 'is', 'apparently', 'assuming', 'that', 'the', 'genre', 'is', 'still', 'hot', 'with', 'the', 'kids', '.', 'it', 'also', 'wrapped', 'production', 'two', 'years', 'ago', 'and', 'has', 'been', 'sitting', 'on', 'the', 'shelves', 'ever', 'since', '.', 'whatever', '.', '.', '.', 'skip', 'it', '!', \"where's\", 'joblo', 'coming', 'from', '?', 'a', 'nightmare', 'of', 'elm', 'street', '3', '(', '7/10', ')', '-', 'blair', 'witch', '2', '(', '7/10', ')', '-', 'the', 'crow', '(', '9/10', ')', '-', 'the', 'crow', ':', 'salvation', '(', '4/10', ')', '-', 'lost', 'highway', '(', '10/10', ')', '-', 'memento', '(', '10/10', ')', '-', 'the', 'others', '(', '9/10', ')', '-', 'stir', 'of', 'echoes', '(', '8/10', ')']\n"
     ]
    }
   ],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    " \n",
    "# load the document\n",
    "filename = 'dataset//txt_sentoken//neg//cv000_29416.txt'\n",
    "text = load_doc(filename)\n",
    "# split into tokens by white space\n",
    "tokens = text.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cc337e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just looking at the raw tokens can give us a lot of ideas of things to try, such as:\n",
    "\"\"\"\n",
    "Remove punctuation from words (e.g. ‘what’s’).\n",
    "Removing tokens that are just punctuation (e.g. ‘-‘).\n",
    "Removing tokens that contain numbers (e.g. ’10/10′).\n",
    "Remove tokens that have one character (e.g. ‘a’).\n",
    "Remove tokens that don’t have much meaning (e.g. ‘and’)\n",
    "Some ideas:\n",
    "\n",
    "We can filter out punctuation from tokens using the string translate() function.\n",
    "We can remove tokens that are just punctuation or contain numbers by using an isalpha() check on each token.\n",
    "We can remove English stop words using the list loaded using NLTK.\n",
    "We can filter out short tokens by checking their length.\n",
    "Below is an updated version of cleaning this review.\n",
    "\"\"\"\n",
    "# Running the example gives a much cleaner looking list of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8957cc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can put this into a function called clean_doc() and test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "211548ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['films', 'adapted', 'comic', 'books', 'plenty', 'success', 'whether', 'theyre', 'superheroes', 'batman', 'superman', 'spawn', 'geared', 'toward', 'kids', 'casper', 'arthouse', 'crowd', 'ghost', 'world', 'theres', 'never', 'really', 'comic', 'book', 'like', 'hell', 'starters', 'created', 'alan', 'moore', 'eddie', 'campbell', 'brought', 'medium', 'whole', 'new', 'level', 'mid', 'series', 'called', 'watchmen', 'say', 'moore', 'campbell', 'thoroughly', 'researched', 'subject', 'jack', 'ripper', 'would', 'like', 'saying', 'michael', 'jackson', 'starting', 'look', 'little', 'odd', 'book', 'graphic', 'novel', 'pages', 'long', 'includes', 'nearly', 'consist', 'nothing', 'footnotes', 'words', 'dont', 'dismiss', 'film', 'source', 'get', 'past', 'whole', 'comic', 'book', 'thing', 'might', 'find', 'another', 'stumbling', 'block', 'hells', 'directors', 'albert', 'allen', 'hughes', 'getting', 'hughes', 'brothers', 'direct', 'seems', 'almost', 'ludicrous', 'casting', 'carrot', 'top', 'well', 'anything', 'riddle', 'better', 'direct', 'film', 'thats', 'set', 'ghetto', 'features', 'really', 'violent', 'street', 'crime', 'mad', 'geniuses', 'behind', 'menace', 'ii', 'society', 'ghetto', 'question', 'course', 'whitechapel', 'londons', 'east', 'end', 'filthy', 'sooty', 'place', 'whores', 'called', 'unfortunates', 'starting', 'get', 'little', 'nervous', 'mysterious', 'psychopath', 'carving', 'profession', 'surgical', 'precision', 'first', 'stiff', 'turns', 'copper', 'peter', 'godley', 'robbie', 'coltrane', 'world', 'enough', 'calls', 'inspector', 'frederick', 'abberline', 'johnny', 'depp', 'blow', 'crack', 'case', 'abberline', 'widower', 'prophetic', 'dreams', 'unsuccessfully', 'tries', 'quell', 'copious', 'amounts', 'absinthe', 'opium', 'upon', 'arriving', 'whitechapel', 'befriends', 'unfortunate', 'named', 'mary', 'kelly', 'heather', 'graham', 'say', 'isnt', 'proceeds', 'investigate', 'horribly', 'gruesome', 'crimes', 'even', 'police', 'surgeon', 'cant', 'stomach', 'dont', 'think', 'anyone', 'needs', 'briefed', 'jack', 'ripper', 'wont', 'go', 'particulars', 'say', 'moore', 'campbell', 'unique', 'interesting', 'theory', 'identity', 'killer', 'reasons', 'chooses', 'slay', 'comic', 'dont', 'bother', 'cloaking', 'identity', 'ripper', 'screenwriters', 'terry', 'hayes', 'vertical', 'limit', 'rafael', 'yglesias', 'les', 'mis', 'rables', 'good', 'job', 'keeping', 'hidden', 'viewers', 'end', 'funny', 'watch', 'locals', 'blindly', 'point', 'finger', 'blame', 'jews', 'indians', 'englishman', 'could', 'never', 'capable', 'committing', 'ghastly', 'acts', 'hells', 'ending', 'whistling', 'stonecutters', 'song', 'simpsons', 'days', 'holds', 'back', 'electric', 'carwho', 'made', 'steve', 'guttenberg', 'star', 'dont', 'worry', 'itll', 'make', 'sense', 'see', 'onto', 'hells', 'appearance', 'certainly', 'dark', 'bleak', 'enough', 'surprising', 'see', 'much', 'looks', 'like', 'tim', 'burton', 'film', 'planet', 'apes', 'times', 'seems', 'like', 'sleepy', 'hollow', 'print', 'saw', 'wasnt', 'completely', 'finished', 'color', 'music', 'finalized', 'comments', 'marilyn', 'manson', 'cinematographer', 'peter', 'deming', 'dont', 'say', 'word', 'ably', 'captures', 'dreariness', 'victorianera', 'london', 'helped', 'make', 'flashy', 'killing', 'scenes', 'remind', 'crazy', 'flashbacks', 'twin', 'peaks', 'even', 'though', 'violence', 'film', 'pales', 'comparison', 'blackandwhite', 'comic', 'oscar', 'winner', 'martin', 'childs', 'shakespeare', 'love', 'production', 'design', 'turns', 'original', 'prague', 'surroundings', 'one', 'creepy', 'place', 'even', 'acting', 'hell', 'solid', 'dreamy', 'depp', 'turning', 'typically', 'strong', 'performance', 'deftly', 'handling', 'british', 'accent', 'ians', 'holm', 'joe', 'goulds', 'secret', 'richardson', 'dalmatians', 'log', 'great', 'supporting', 'roles', 'big', 'surprise', 'graham', 'cringed', 'first', 'time', 'opened', 'mouth', 'imagining', 'attempt', 'irish', 'accent', 'actually', 'wasnt', 'half', 'bad', 'film', 'however', 'good', 'strong', 'violencegore', 'sexuality', 'language', 'drug', 'content']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    " \n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    " \n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens\n",
    " \n",
    "# load the document\n",
    "filename = 'dataset//txt_sentoken//pos//cv000_29590.txt'\n",
    "text = load_doc(filename)\n",
    "tokens = clean_doc(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d425b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Develop Vocabulary\n",
    "# When working with predictive models of text, like a bag-of-words model, there is a pressure to \n",
    "# reduce the size of the vocabulary.\n",
    "# The larger the vocabulary, the more sparse the representation of each word or document.\n",
    "\n",
    "# A part of preparing text for sentiment analysis involves defining and tailoring the vocabulary of \n",
    "# words supported by the model.\n",
    "\n",
    "# We can do this by loading all of the documents in the dataset and building a set of words. \n",
    "# We may decide to support all of these words, or perhaps discard some. \n",
    "# The final chosen vocabulary can then be saved to file for later use, such as filtering words in new documents in the future.\n",
    "\n",
    "# We can keep track of the vocabulary in a Counter, which is a dictionary of words and their count with \n",
    "# some additional convenience functions.\n",
    "\n",
    "# We will develop a function to process a document and add it to the vocabulary. \n",
    "# The function needs to load a document by calling the previously developed load_doc() function. \n",
    "# It needs to clean the loaded document using the previously developed clean_doc() function, then it needs to\n",
    "# add all the tokens to the Counter, and update counts. We do this by calling the update() function on the counter object.\n",
    "\n",
    "# Below is a function called add_doc_to_vocab() that takes as arguments a document filename and a Counter vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0a30b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc and add to vocab\n",
    "def add_doc_to_vocab(filename, vocab):\n",
    "    # load doc\n",
    "    doc = load_doc(filename)\n",
    "    # clean doc\n",
    "    tokens = clean_doc(doc)\n",
    "    # update counts\n",
    "    vocab.update(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f35d90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab):\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # skip files that do not have the right extension\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # add doc to vocab\n",
    "        add_doc_to_vocab(path, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0286f64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting all together we can develop a full vocabulary from all documents in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63bc44fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46557\n",
      "[('film', 8860), ('one', 5521), ('movie', 5440), ('like', 3553), ('even', 2555), ('good', 2320), ('time', 2283), ('story', 2118), ('films', 2102), ('would', 2042), ('much', 2024), ('also', 1965), ('characters', 1947), ('get', 1921), ('character', 1906), ('two', 1825), ('first', 1768), ('see', 1730), ('well', 1694), ('way', 1668), ('make', 1590), ('really', 1563), ('little', 1491), ('life', 1472), ('plot', 1451), ('people', 1420), ('movies', 1416), ('could', 1395), ('bad', 1374), ('scene', 1373), ('never', 1364), ('best', 1301), ('new', 1277), ('many', 1268), ('doesnt', 1267), ('man', 1266), ('scenes', 1265), ('dont', 1210), ('know', 1207), ('hes', 1150), ('great', 1141), ('another', 1111), ('love', 1089), ('action', 1078), ('go', 1075), ('us', 1065), ('director', 1056), ('something', 1048), ('end', 1047), ('still', 1038)]\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    " \n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens\n",
    "\n",
    "# load doc and add to vocab\n",
    "def add_doc_to_vocab(filename, vocab):\n",
    "    # load doc\n",
    "    doc = load_doc(filename)\n",
    "    # clean doc\n",
    "    tokens = clean_doc(doc)\n",
    "    # update counts\n",
    "    vocab.update(tokens)\n",
    "\n",
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab):\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # skip files that do not have the right extension\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # add doc to vocab\n",
    "        add_doc_to_vocab(path, vocab)\n",
    "\n",
    "# define vocab\n",
    "vocab = Counter()\n",
    "# add all docs to vocab\n",
    "process_docs('dataset//txt_sentoken//neg', vocab)\n",
    "process_docs('dataset//txt_sentoken//pos', vocab)\n",
    "# print the size of the vocab\n",
    "print(len(vocab))\n",
    "# print the top words in the vocab\n",
    "print(vocab.most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38b89ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This created a vocabulary with all documents in the dataset.\n",
    "# We can see that there are a little over 46,000 unique words across all reviews and \n",
    "# the top 3 words are ‘film‘, ‘one‘, and ‘movie‘.\n",
    "\n",
    "# Perhaps the least common words, those that only appear once across all reviews, are not predictive. \n",
    "# Perhaps some of the most common words are not useful too.\n",
    "# These are good questions and really should be tested with a specific predictive model.\n",
    "\n",
    "# Generally, words that only appear once or a few times across 2,000 reviews are probably not predictive \n",
    "# and can be removed from the vocabulary, greatly cutting down on the tokens we need to model.\n",
    "\n",
    "# We can do this by stepping through words and their counts and only keeping those with a count above a \n",
    "# chosen threshold. Here we will use 5 occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1d2ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep tokens with > 5 occurrence\n",
    "min_occurane = 5\n",
    "tokens = [k for k,c in vocab.items() if c >= min_occurane]\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790a7197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This reduces the vocabulary from 46,557 to 14,803 words, a huge drop. Perhaps a minimum of 5 occurrences \n",
    "# is too aggressive; you can experiment with different values.\n",
    "\n",
    "# We can then save the chosen vocabulary of words to a new file. I like to save the vocabulary as ASCII with one word per line.\n",
    "\n",
    "# Below defines a function called save_list() to save a list of items, in this case, tokens to file, one per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a36d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_list(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4e82b4",
   "metadata": {},
   "source": [
    "# Complete example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d0a1285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46557\n",
      "[('film', 8860), ('one', 5521), ('movie', 5440), ('like', 3553), ('even', 2555), ('good', 2320), ('time', 2283), ('story', 2118), ('films', 2102), ('would', 2042), ('much', 2024), ('also', 1965), ('characters', 1947), ('get', 1921), ('character', 1906), ('two', 1825), ('first', 1768), ('see', 1730), ('well', 1694), ('way', 1668), ('make', 1590), ('really', 1563), ('little', 1491), ('life', 1472), ('plot', 1451), ('people', 1420), ('movies', 1416), ('could', 1395), ('bad', 1374), ('scene', 1373), ('never', 1364), ('best', 1301), ('new', 1277), ('many', 1268), ('doesnt', 1267), ('man', 1266), ('scenes', 1265), ('dont', 1210), ('know', 1207), ('hes', 1150), ('great', 1141), ('another', 1111), ('love', 1089), ('action', 1078), ('go', 1075), ('us', 1065), ('director', 1056), ('something', 1048), ('end', 1047), ('still', 1038)]\n",
      "14803\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    " \n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens\n",
    "\n",
    "# load doc and add to vocab\n",
    "def add_doc_to_vocab(filename, vocab):\n",
    "    # load doc\n",
    "    doc = load_doc(filename)\n",
    "    # clean doc\n",
    "    tokens = clean_doc(doc)\n",
    "    # update counts\n",
    "    vocab.update(tokens)\n",
    "\n",
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab):\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # skip files that do not have the right extension\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # add doc to vocab\n",
    "        add_doc_to_vocab(path, vocab)\n",
    "\n",
    "# save list to file\n",
    "def save_list(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "\n",
    "# define vocab\n",
    "vocab = Counter()\n",
    "# add all docs to vocab\n",
    "process_docs('dataset//txt_sentoken//neg', vocab)\n",
    "process_docs('dataset//txt_sentoken//pos', vocab)\n",
    "# print the size of the vocab\n",
    "print(len(vocab))\n",
    "# print the top words in the vocab\n",
    "print(vocab.most_common(50))\n",
    "# keep tokens with > 5 occurrence\n",
    "min_occurane = 5\n",
    "tokens = [k for k,c in vocab.items() if c >= min_occurane]\n",
    "print(len(tokens))\n",
    "# save tokens to a vocabulary file\n",
    "save_list(tokens, 'vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9ed562a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next step: Save Prepared Data\n",
    "# We can use the data cleaning and chosen vocabulary to prepare each movie review and save the prepared \n",
    "# versions of the reviews ready for modeling.\n",
    "\n",
    "# This is a good practice as it decouples the data preparation from modeling, allowing you to focus on modeling \n",
    "# and circle back to data prep if you have new ideas.\n",
    "\n",
    "# We can start off by loading the vocabulary from ‘vocab.txt‘.\n",
    "\n",
    "# Next, we can clean the reviews, use the loaded vocab to filter out unwanted tokens, and save the clean reviews in a new file.\n",
    "\n",
    "# One approach could be to save all the positive reviews in one file and all the negative reviews in another file, \n",
    "# with the filtered tokens separated by white space for each review on separate lines.\n",
    "\n",
    "# First, we can define a function to process a document, clean it, filter it, and return it as a single line that \n",
    "# could be saved in a file. Below defines the doc_to_line() function to do just that, taking a filename and \n",
    "# vocabulary (as a set) as arguments.\n",
    "\n",
    "# It calls the previously defined load_doc() function to load the document and clean_doc() to tokenize the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abd02cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc, clean and return line of tokens\n",
    "def doc_to_line(filename, vocab):\n",
    "    # load the doc\n",
    "    doc = load_doc(filename)\n",
    "    # clean doc\n",
    "    tokens = clean_doc(doc)\n",
    "    # filter by vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee01edf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we can define a new version of process_docs() to step through all reviews in a folder and convert them \n",
    "# to lines by calling doc_to_line() for each document. \n",
    "# A list of lines is then returned.\n",
    "# We can then call process_docs() for both the directories of positive and negative reviews, then call \n",
    "# save_list() from the previous section to save each list of processed reviews to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e852b5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab):\n",
    "    lines = list()\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # skip files that do not have the right extension\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '//' + filename\n",
    "        # load and clean the doc\n",
    "        line = doc_to_line(path, vocab)\n",
    "        # add to list\n",
    "        lines.append(line)\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc0c40a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The complete code listing is provided below.\n",
    "# Running the example saves two new files, ‘negative.txt‘ and ‘positive.txt‘, that contain the prepared \n",
    "# negative and positive reviews respectively.\n",
    "\n",
    "# The data is then ready for use in a bag-of-words or even word embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "393246a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    " \n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    " \n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens\n",
    " \n",
    "# save list to file\n",
    "def save_list(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    " \n",
    "# load doc, clean and return line of tokens\n",
    "def doc_to_line(filename, vocab):\n",
    "    # load the doc\n",
    "    doc = load_doc(filename)\n",
    "    # clean doc\n",
    "    tokens = clean_doc(doc)\n",
    "    # filter by vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    return ' '.join(tokens)\n",
    " \n",
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab):\n",
    "    lines = list()\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # skip files that do not have the right extension\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '//' + filename\n",
    "        # load and clean the doc\n",
    "        line = doc_to_line(path, vocab)\n",
    "        # add to list\n",
    "        lines.append(line)\n",
    "    return lines\n",
    " \n",
    "# load vocabulary\n",
    "vocab_filename = 'vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)\n",
    "# prepare negative reviews\n",
    "negative_lines = process_docs('dataset//txt_sentoken//neg', vocab)\n",
    "save_list(negative_lines, 'negative.txt')\n",
    "# prepare positive reviews\n",
    "positive_lines = process_docs('dataset//txt_sentoken//pos', vocab)\n",
    "save_list(positive_lines, 'positive.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20df1e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/develop-word-embedding-model-predicting-movie-review-sentiment/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e915102d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Embedding Layer\n",
    "# A word embedding is a way of representing text where each word in the vocabulary is represented by a \n",
    "# real valued vector in a high-dimensional space. The vectors are learned in such a way that words that \n",
    "# have similar meanings will have similar representation in the vector space (close in the vector space). \n",
    "# This is a more expressive representation for text than more classical methods like bag-of-words, where \n",
    "# relationships between words or tokens are ignored, or forced in bigram and trigram approaches.\n",
    "\n",
    "# The real valued vector representation for words can be learned while training the neural network. \n",
    "# We can do this in the Keras deep learning library using the Embedding layer.\n",
    "\n",
    "# The first step is to load the vocabulary. We will use it to filter out words from movie reviews that \n",
    "# we are not interested in.\n",
    "# If you have worked through the previous section, you should have a local file called ‘vocab.txt‘ \n",
    "# with one word per line. We can load that file and build a vocabulary as a set for checking the validity of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bb0886b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    " \n",
    "# load the vocabulary\n",
    "vocab_filename = 'vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0ce2ccd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we need to load all of the training data movie reviews. For that we can adapt the process_docs() \n",
    "# rom the previous section to load the documents, clean them, and return them as a list of strings, with \n",
    "# one document per string. We want each document to be a string for easy encoding as a sequence of integers later.\n",
    "# Cleaning the document involves splitting each review based on white space, removing punctuation, and then filtering \n",
    "# out all tokens not in the vocabulary.\n",
    "\n",
    "# The updated clean_doc() function is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "81cf4a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc, vocab):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # filter out tokens not in vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "95639328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The updated process_docs() can then call the clean_doc() for each document on the ‘pos‘ and ‘neg‘ directories \n",
    "# that are in our training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "de212bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab, is_trian):\n",
    "    documents = list()\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # skip any reviews in the test set\n",
    "        if is_trian and filename.startswith('cv9'):\n",
    "            continue\n",
    "        if not is_trian and not filename.startswith('cv9'):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load the doc\n",
    "        doc = load_doc(path)\n",
    "        # clean doc\n",
    "        tokens = clean_doc(doc, vocab)\n",
    "        # add to list\n",
    "        documents.append(tokens)\n",
    "    return documents\n",
    " \n",
    "# load all training reviews\n",
    "positive_docs = process_docs('dataset//txt_sentoken//pos', vocab, True)\n",
    "negative_docs = process_docs('dataset//txt_sentoken//neg', vocab, True)\n",
    "train_docs = negative_docs + positive_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e3725d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['plot two teen couples go church party drink drive get accident one guys dies girlfriend continues see life nightmares whats deal watch movie sorta find critique movie teen generation touches cool idea presents bad package makes review even harder one write since generally applaud films attempt break mold mess head lost highway memento good bad ways making types films folks didnt one correctly seem taken pretty neat concept executed terribly problems movie well main problem simply jumbled starts normal fantasy world audience member idea whats going dreams characters coming back dead others look like dead strange apparitions chase scenes tons weird things happen simply explained personally dont mind trying unravel film every give clue get kind fed films biggest problem obviously got big secret hide seems want hide completely final five minutes make things entertaining thrilling even engaging meantime really sad part arrow dig flicks like actually figured halfway point strangeness start make little bit sense still didnt make film entertaining guess bottom line movies like always make sure audience even given secret enter world understanding mean showing melissa sagemiller running away visions minutes throughout movie plain lazy okay get people chasing dont know really need see giving us different scenes offering insight strangeness going movie apparently studio took film away director shows mightve pretty decent teen movie somewhere guess suits decided turning music video little edge would make sense actors pretty good part although wes bentley seemed playing exact character american beauty new neighborhood biggest kudos go sagemiller holds throughout entire film actually feeling characters overall film doesnt stick doesnt entertain confusing rarely feels pretty redundant runtime despite pretty cool ending explanation came oh way horror teen slasher flick look way someone apparently assuming genre still hot kids also wrapped production two years ago sitting shelves ever since whatever skip wheres joblo coming nightmare elm street blair witch crow crow salvation lost highway memento others stir echoes']\n"
     ]
    }
   ],
   "source": [
    "print(train_docs[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6ce05761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next step is to encode each document as a sequence of integers.\n",
    "# The Keras Embedding layer requires integer inputs where each integer maps to a single token that has a specific \n",
    "# real-valued vector representation within the embedding. These vectors are random at the beginning of training, \n",
    "# but during training become meaningful to the network.\n",
    "\n",
    "# We can encode the training documents as sequences of integers using the Tokenizer class in the Keras API.\n",
    "# First, we must construct an instance of the class then train it on all documents in the training dataset. \n",
    "# In this case, it develops a vocabulary of all tokens in the training dataset and develops a consistent mapping\n",
    "# from words in the vocabulary to unique integers. We could just as easily develop this mapping ourselves using \n",
    "# our vocabulary file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e428edb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "# create the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "tokenizer.fit_on_texts(train_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "56b4827e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that the mapping of words to integers has been prepared, we can use it to encode the reviews in the training dataset. \n",
    "# We can do that by calling the texts_to_sequences() function on the Tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a3c092ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence encode\n",
    "encoded_docs = tokenizer.texts_to_sequences(train_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a8d2b716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also need to ensure that all documents have the same length.\n",
    "\n",
    "# This is a requirement of Keras for efficient computation. We could truncate reviews to the smallest size or \n",
    "# zero-pad (pad with the value ‘0’) reviews to the maximum length, or some hybrid. In this case, we will pad \n",
    "# all reviews to the length of the longest review in the training dataset.\n",
    "\n",
    "# First, we can find the longest review using the max() function on the training dataset and take its length. \n",
    "# We can then call the Keras function pad_sequences() to pad the sequences to the maximum length by adding 0 values on the end.\n",
    "\n",
    "# Next, we can define the class labels for the training dataset, needed to fit the supervised neural network \n",
    "# model to predict the sentiment of reviews.\n",
    "\n",
    "# We can then encode and pad the test dataset, needed later to evaluate the model after we train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "448f87f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from numpy import array\n",
    "\n",
    "# pad sequences\n",
    "max_length = max([len(s.split()) for s in train_docs])\n",
    "Xtrain = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "\n",
    "# define training labels\n",
    "ytrain = array([0 for _ in range(100)] + [1 for _ in range(100)])\n",
    "\n",
    "# load all test reviews\n",
    "positive_docs = process_docs('dataset//txt_sentoken//pos', vocab, False)\n",
    "negative_docs = process_docs('dataset//txt_sentoken//neg', vocab, False)\n",
    "test_docs = negative_docs + positive_docs\n",
    "# sequence encode\n",
    "encoded_docs = tokenizer.texts_to_sequences(test_docs)\n",
    "# pad sequences\n",
    "Xtest = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "# define test labels\n",
    "ytest = array([0 for _ in range(100)] + [1 for _ in range(100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ebded071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 940)\n"
     ]
    }
   ],
   "source": [
    "print(Xtrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c5a65f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200,)\n"
     ]
    }
   ],
   "source": [
    "print(ytrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "723f0853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 940)\n"
     ]
    }
   ],
   "source": [
    "print(Xtest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4eb1157f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200,)\n"
     ]
    }
   ],
   "source": [
    "print(ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d5ae3e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are now ready to define our neural network model.\n",
    "\n",
    "# The model will use an Embedding layer as the first hidden layer. \n",
    "# he Embedding requires the specification of the vocabulary size, the size of the real-valued vector space, \n",
    "# and the maximum length of input documents.\n",
    "\n",
    "# The vocabulary size is the total number of words in our vocabulary, plus one for unknown words. \n",
    "# This could be the vocab set length or the size of the vocab within the tokenizer used to integer encode the documents, \n",
    "# for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d38730a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define vocabulary size (largest integer value)\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f9b3f0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use a 100-dimensional vector space, but you could try other values, such as 50 or 150. Finally, the maximum document length was calculated above in the max_length variable used during padding.\n",
    "\n",
    "# The complete model definition is listed below including the Embedding layer.\n",
    "# We use a Convolutional Neural Network (CNN) as they have proven to be successful at document classification problems. \n",
    "# A conservative CNN configuration is used with 32 filters (parallel fields for processing words) and a kernel size of \n",
    "# 8 with a rectified linear (‘relu’) activation function. This is followed by a pooling layer that reduces the output \n",
    "# of the convolutional layer by half.\n",
    "\n",
    "# Next, the 2D output from the CNN part of the model is flattened to one long 2D vector to represent the ‘features’ \n",
    "# extracted by the CNN. The back-end of the model is a standard Multilayer Perceptron layers to interpret the CNN features. \n",
    "# The output layer uses a sigmoid activation function to output a value between 0 and 1 for the negative and positive \n",
    "# sentiment in the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f1a00033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_5 (Embedding)     (None, 1244, 100)         1478100   \n",
      "                                                                 \n",
      " conv1d_4 (Conv1D)           (None, 1237, 32)          25632     \n",
      "                                                                 \n",
      " max_pooling1d_4 (MaxPooling  (None, 618, 32)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 19776)             0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 10)                197770    \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,701,513\n",
      "Trainable params: 1,701,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
    "model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cdc18776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running just this piece provides a summary of the defined network.\n",
    "\n",
    "# We can see that the Embedding layer expects documents with a length of 442 words as input and encodes \n",
    "# each word in the document as a 100 element vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "53d8248a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we fit the network on the training data.\n",
    "\n",
    "# We use a binary cross entropy loss function because the problem we are learning is a binary classification problem. \n",
    "# The efficient Adam implementation of stochastic gradient descent is used and we keep track of accuracy in addition \n",
    "# to loss during training. The model is trained for 10 epochs, or 10 passes through the training data.\n",
    "\n",
    "# The network configuration and training schedule were found with a little trial and error, but are by no means \n",
    "# optimal for this problem. If you can get better results with a different configuration, let me know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "857edd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "57/57 - 6s - loss: 0.6892 - accuracy: 0.5322 - 6s/epoch - 104ms/step\n",
      "Epoch 2/10\n",
      "57/57 - 6s - loss: 0.4968 - accuracy: 0.7939 - 6s/epoch - 98ms/step\n",
      "Epoch 3/10\n",
      "57/57 - 6s - loss: 0.1010 - accuracy: 0.9678 - 6s/epoch - 97ms/step\n",
      "Epoch 4/10\n",
      "57/57 - 5s - loss: 0.0124 - accuracy: 0.9994 - 5s/epoch - 96ms/step\n",
      "Epoch 5/10\n",
      "57/57 - 5s - loss: 0.0033 - accuracy: 1.0000 - 5s/epoch - 96ms/step\n",
      "Epoch 6/10\n",
      "57/57 - 6s - loss: 0.0019 - accuracy: 1.0000 - 6s/epoch - 98ms/step\n",
      "Epoch 7/10\n",
      "57/57 - 6s - loss: 0.0013 - accuracy: 1.0000 - 6s/epoch - 100ms/step\n",
      "Epoch 8/10\n",
      "57/57 - 6s - loss: 0.0010 - accuracy: 1.0000 - 6s/epoch - 103ms/step\n",
      "Epoch 9/10\n",
      "57/57 - 6s - loss: 8.0440e-04 - accuracy: 1.0000 - 6s/epoch - 98ms/step\n",
      "Epoch 10/10\n",
      "57/57 - 6s - loss: 6.6614e-04 - accuracy: 1.0000 - 6s/epoch - 99ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ecc6845b80>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile network\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "model.fit(Xtrain, ytrain, epochs=10, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "573b44c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After the model is fit, it is evaluated on the test dataset. This dataset contains words that we have not \n",
    "# seen before and reviews not seen during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "857641eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 88.000000\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "dde278d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can tie all of this together.\n",
    "\n",
    "# The complete code listing is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ee74a7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_7 (Embedding)     (None, 940, 100)          990300    \n",
      "                                                                 \n",
      " conv1d_6 (Conv1D)           (None, 933, 32)           25632     \n",
      "                                                                 \n",
      " max_pooling1d_6 (MaxPooling  (None, 466, 32)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " flatten_5 (Flatten)         (None, 14912)             0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 10)                149130    \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,165,073\n",
      "Trainable params: 1,165,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "7/7 - 1s - loss: 0.7029 - accuracy: 0.5050 - 927ms/epoch - 132ms/step\n",
      "Epoch 2/10\n",
      "7/7 - 0s - loss: 0.6595 - accuracy: 0.7900 - 488ms/epoch - 70ms/step\n",
      "Epoch 3/10\n",
      "7/7 - 0s - loss: 0.5900 - accuracy: 0.8100 - 461ms/epoch - 66ms/step\n",
      "Epoch 4/10\n",
      "7/7 - 1s - loss: 0.4547 - accuracy: 0.9900 - 509ms/epoch - 73ms/step\n",
      "Epoch 5/10\n",
      "7/7 - 0s - loss: 0.2831 - accuracy: 0.9950 - 464ms/epoch - 66ms/step\n",
      "Epoch 6/10\n",
      "7/7 - 0s - loss: 0.1332 - accuracy: 1.0000 - 460ms/epoch - 66ms/step\n",
      "Epoch 7/10\n",
      "7/7 - 0s - loss: 0.0557 - accuracy: 1.0000 - 466ms/epoch - 67ms/step\n",
      "Epoch 8/10\n",
      "7/7 - 0s - loss: 0.0224 - accuracy: 1.0000 - 465ms/epoch - 66ms/step\n",
      "Epoch 9/10\n",
      "7/7 - 1s - loss: 0.0118 - accuracy: 1.0000 - 506ms/epoch - 72ms/step\n",
      "Epoch 10/10\n",
      "7/7 - 0s - loss: 0.0067 - accuracy: 1.0000 - 473ms/epoch - 68ms/step\n",
      "Test Accuracy: 100.000000\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "from os import listdir\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    " \n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    " \n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc, vocab):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # filter out tokens not in vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens\n",
    " \n",
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab, is_trian):\n",
    "    documents = list()\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # skip any reviews in the test set\n",
    "        if is_trian and filename.startswith('cv9'):\n",
    "            continue\n",
    "        if not is_trian and not filename.startswith('cv9'):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load the doc\n",
    "        doc = load_doc(path)\n",
    "        # clean doc\n",
    "        tokens = clean_doc(doc, vocab)\n",
    "        # add to list\n",
    "        documents.append(tokens)\n",
    "    return documents\n",
    " \n",
    "# load the vocabulary\n",
    "vocab_filename = 'vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)\n",
    " \n",
    "# load all training reviews\n",
    "positive_docs = process_docs('dataset//txt_sentoken//pos', vocab, False)\n",
    "negative_docs = process_docs('dataset//txt_sentoken//neg', vocab, False)\n",
    "train_docs = negative_docs + positive_docs\n",
    " \n",
    "# create the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "tokenizer.fit_on_texts(train_docs)\n",
    " \n",
    "# sequence encode\n",
    "encoded_docs = tokenizer.texts_to_sequences(train_docs)\n",
    "    \n",
    " # pad sequences\n",
    "max_length = max([len(s.split()) for s in train_docs])\n",
    "Xtrain = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "# define training labels\n",
    "ytrain = array([0 for _ in range(100)] + [1 for _ in range(100)])\n",
    "\n",
    "# load all test reviews\n",
    "positive_docs = process_docs('dataset//txt_sentoken//pos', vocab, False)\n",
    "negative_docs = process_docs('dataset//txt_sentoken//neg', vocab, False)\n",
    "test_docs = negative_docs + positive_docs\n",
    "\n",
    "# sequence encode\n",
    "encoded_docs = tokenizer.texts_to_sequences(test_docs)\n",
    "\n",
    "# pad sequences\n",
    "Xtest = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "# define test labels\n",
    "ytest = array([0 for _ in range(100)] + [1 for _ in range(100)])   \n",
    " \n",
    "# define vocabulary size (largest integer value)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
    "model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())\n",
    "\n",
    "# compile network\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "model.fit(Xtrain, ytrain, epochs=10, verbose=2)\n",
    "# evaluate\n",
    "loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e299f6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the example prints the loss and accuracy at the end of each training epoch.\n",
    "\n",
    "# Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, \n",
    "# or differences in numerical precision. Consider running the example a few times and compare the average outcome.\n",
    "\n",
    "# We can see that the model very quickly achieves 100% accuracy on the training dataset. At the end of the run, \n",
    "# the model achieves an accuracy of 84.5% on the test dataset, which is a great score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f876121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have just seen an example of how we can learn a word embedding as part of fitting a neural network model.\n",
    "# Next, let’s look at how we can efficiently learn a standalone embedding that we could later use in our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf4def5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train word2vec Embedding\n",
    "\n",
    "# A downside of learning a word embedding as part of the network is that it can be very slow, especially for very large \n",
    "# text datasets.\n",
    "# The word2vec algorithm is an approach to learning a word embedding from a text corpus in a standalone way. \n",
    "# The benefit of the method is that it can produce high-quality word embeddings very efficiently, in terms of space and \n",
    "# time complexity.\n",
    "\n",
    "# The first step is to prepare the documents ready for learning the embedding. This involves the same data cleaning steps \n",
    "# from the previous section, namely splitting documents by their white space, removing punctuation, and filtering out tokens \n",
    "# not in the vocabulary.\n",
    "\n",
    "# The word2vec algorithm processes documents sentence by sentence. This means we will preserve the sentence-based \n",
    "# structure during cleaning.\n",
    "\n",
    "# We start by loading the vocabulary, as before.\n",
    "# Next, we define a function named doc_to_clean_lines() to clean a loaded document line by line and return a list of the \n",
    "# cleaned lines.\n",
    "# Next, we adapt the process_docs() function to load and clean all of the documents in a folder and return a list of all \n",
    "# document lines.\n",
    "\n",
    "# The results from this function will be the training data for the word2vec model.\n",
    "# We can then load all of the training data and convert it into a long list of ‘sentences’ (lists of tokens) ready for fitting \n",
    "# the word2vec model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bb2fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the word2vec implementation provided in the Gensim Python library. Specifically the Word2Vec class.\n",
    "# The model is fit when constructing the class. We pass in the list of clean sentences from the training data, \n",
    "# then specify the size of the embedding vector space (we use 100 again), the number of neighboring words to look at \n",
    "# when learning how to embed each word in the training sentences (we use 5 neighbors), the number of threads to use \n",
    "# when fitting the model (we use 8, but change this if you have more or less CPU cores), and the minimum occurrence \n",
    "# count for words to consider in the vocabulary (we set this to 1 as we have already prepared the vocabulary).\n",
    "\n",
    "# After the model is fit, we print the size of the learned vocabulary, which should match the size of our vocabulary \n",
    "# in vocab.txt of 25,767 tokens.\n",
    "# Finally, we save the learned embedding vectors to file using the save_word2vec_format() on the model’s ‘wv‘ (word vector) \n",
    "# attribute. The embedding is saved in ASCII format with one word and vector per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b0348d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training sentences: 31996\n",
      "Vocabulary size: 14254\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "from os import listdir\n",
    "from gensim.models import Word2Vec\n",
    " \n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    " \n",
    "# turn a doc into clean tokens\n",
    "def doc_to_clean_lines(doc, vocab):\n",
    "    clean_lines = list()\n",
    "    lines = doc.splitlines()\n",
    "    for line in lines:\n",
    "        # split into tokens by white space\n",
    "        tokens = line.split()\n",
    "        # remove punctuation from each token\n",
    "        table = str.maketrans('', '', punctuation)\n",
    "        tokens = [w.translate(table) for w in tokens]\n",
    "        # filter out tokens not in vocab\n",
    "        tokens = [w for w in tokens if w in vocab]\n",
    "        clean_lines.append(tokens)\n",
    "    return clean_lines\n",
    " \n",
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab, is_trian):\n",
    "    lines = list()\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # skip any reviews in the test set\n",
    "        if is_trian and filename.startswith('cv9'):\n",
    "            continue\n",
    "        if not is_trian and not filename.startswith('cv9'):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load and clean the doc\n",
    "        doc = load_doc(path)\n",
    "        doc_lines = doc_to_clean_lines(doc, vocab)\n",
    "        # add lines to list\n",
    "        lines += doc_lines\n",
    "    return lines\n",
    " \n",
    "# load the vocabulary\n",
    "vocab_filename = 'vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)\n",
    " \n",
    "# load training data\n",
    "positive_docs = process_docs('dataset//txt_sentoken//pos', vocab, False)\n",
    "negative_docs = process_docs('dataset//txt_sentoken//neg', vocab, True)\n",
    "sentences = negative_docs + positive_docs\n",
    "print('Total training sentences: %d' % len(sentences))\n",
    " \n",
    "# train word2vec model\n",
    "# check newest verision at https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, workers=8, min_count=1)\n",
    "# summarize vocabulary size in model\n",
    "# previos version of Gensim used: words = list(model.wv.vocab)\n",
    "words = list(model.wv.index_to_key)\n",
    "print('Vocabulary size: %d' % len(words))\n",
    " \n",
    "# save model in ASCII (word2vec) format\n",
    "filename = 'embedding_word2vec.txt'\n",
    "model.wv.save_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "fee0b57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Pre-trained Embedding\n",
    "\n",
    "# We can use the pre-trained word embedding developed in the previous section and the CNN model developed in the \n",
    "# section before that.\n",
    "\n",
    "# The first step is to load the word embedding as a directory of words to vectors. The word embedding was saved \n",
    "# in so-called ‘word2vec‘ format that contains a header line. We will skip this header line when loading the embedding.\n",
    "\n",
    "# The function named load_embedding() loads the embedding and returns a directory of words mapped to the vectors in NumPy format.\n",
    "# Now with all of the vectors in memory, we can order them in such a way as to match the integer encoding prepared by the \n",
    "# Keras Tokenizer.\n",
    "# Recall that we integer encode the review documents prior to passing them to the Embedding layer. The integer maps to \n",
    "# the index of a specific vector in the embedding layer. Therefore, it is important that we lay the vectors out in the \n",
    "# Embedding layer such that the encoded words map to the correct vector.\n",
    "# The function get_weight_matrix() takes the loaded embedding and the tokenizer.word_index vocabulary as arguments and \n",
    "# returns a matrix with the word vectors in the correct locations.\n",
    "# Next we use these functions to create our new Embedding layer for our model.\n",
    "\n",
    "# Note that the prepared weight matrix embedding_vectors is passed to the new Embedding layer as an argument and that \n",
    "# we set the ‘trainable‘ argument to ‘False‘ to ensure that the network does not try to adapt the pre-learned vectors \n",
    "# as part of training the network.\n",
    "\n",
    "# We add this layer to our model. We also have a slightly different model configuration with a lot more filters (128) \n",
    "# in the CNN model and a kernel that matches the 5 words used as neighbors when developing the word2vec embedding. \n",
    "# Finally, the back-end of the model was simplified. These changes were found with a little trial and error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "70f0a54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_8 (Embedding)     (None, 1244, 100)         1478100   \n",
      "                                                                 \n",
      " conv1d_7 (Conv1D)           (None, 1240, 128)         64128     \n",
      "                                                                 \n",
      " max_pooling1d_7 (MaxPooling  (None, 620, 128)         0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         (None, 79360)             0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 1)                 79361     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,621,589\n",
      "Trainable params: 143,489\n",
      "Non-trainable params: 1,478,100\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "57/57 - 6s - loss: nan - accuracy: 0.5050 - 6s/epoch - 106ms/step\n",
      "Epoch 2/10\n",
      "57/57 - 6s - loss: nan - accuracy: 0.5000 - 6s/epoch - 99ms/step\n",
      "Epoch 3/10\n",
      "57/57 - 6s - loss: nan - accuracy: 0.5000 - 6s/epoch - 100ms/step\n",
      "Epoch 4/10\n",
      "57/57 - 6s - loss: nan - accuracy: 0.5000 - 6s/epoch - 99ms/step\n",
      "Epoch 5/10\n",
      "57/57 - 6s - loss: nan - accuracy: 0.5000 - 6s/epoch - 101ms/step\n",
      "Epoch 6/10\n",
      "57/57 - 6s - loss: nan - accuracy: 0.5000 - 6s/epoch - 100ms/step\n",
      "Epoch 7/10\n",
      "57/57 - 6s - loss: nan - accuracy: 0.5000 - 6s/epoch - 105ms/step\n",
      "Epoch 8/10\n",
      "57/57 - 6s - loss: nan - accuracy: 0.5000 - 6s/epoch - 103ms/step\n",
      "Epoch 9/10\n",
      "57/57 - 6s - loss: nan - accuracy: 0.5000 - 6s/epoch - 104ms/step\n",
      "Epoch 10/10\n",
      "57/57 - 6s - loss: nan - accuracy: 0.5000 - 6s/epoch - 104ms/step\n",
      "Test Accuracy: 50.000000\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "from os import listdir\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    " \n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    " \n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc, vocab):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # filter out tokens not in vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens\n",
    " \n",
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab, is_trian):\n",
    "    documents = list()\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # skip any reviews in the test set\n",
    "        if is_trian and filename.startswith('cv9'):\n",
    "            continue\n",
    "        if not is_trian and not filename.startswith('cv9'):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load the doc\n",
    "        doc = load_doc(path)\n",
    "        # clean doc\n",
    "        tokens = clean_doc(doc, vocab)\n",
    "        # add to list\n",
    "        documents.append(tokens)\n",
    "    return documents\n",
    " \n",
    "# load embedding as a dict\n",
    "def load_embedding(filename):\n",
    "    # load embedding into memory, skip first line\n",
    "    file = open(filename,'r')\n",
    "    lines = file.readlines()[1:]\n",
    "    file.close()\n",
    "    # create a map of words to vectors\n",
    "    embedding = dict()\n",
    "    for line in lines:\n",
    "        parts = line.split()\n",
    "        # key is string word, value is numpy array for vector\n",
    "        embedding[parts[0]] = asarray(parts[1:], dtype='float32')\n",
    "    return embedding\n",
    " \n",
    "# create a weight matrix for the Embedding layer from a loaded embedding\n",
    "def get_weight_matrix(embedding, vocab):\n",
    "    # total vocabulary size plus 0 for unknown words\n",
    "    vocab_size = len(vocab) + 1\n",
    "    # define weight matrix dimensions with all 0\n",
    "    weight_matrix = zeros((vocab_size, 100))\n",
    "    # step vocab, store vectors using the Tokenizer's integer mapping\n",
    "    for word, i in vocab.items():\n",
    "        weight_matrix[i] = embedding.get(word)\n",
    "    return weight_matrix\n",
    " \n",
    "# load the vocabulary\n",
    "vocab_filename = 'vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)\n",
    " \n",
    "# load all training reviews\n",
    "positive_docs = process_docs('dataset//txt_sentoken//pos', vocab, True)\n",
    "negative_docs = process_docs('dataset//txt_sentoken//neg', vocab, True)\n",
    "train_docs = negative_docs + positive_docs\n",
    " \n",
    "# create the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "tokenizer.fit_on_texts(train_docs)\n",
    " \n",
    "# sequence encode\n",
    "encoded_docs = tokenizer.texts_to_sequences(train_docs)\n",
    "# pad sequences\n",
    "max_length = max([len(s.split()) for s in train_docs])\n",
    "Xtrain = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "# define training labels\n",
    "ytrain = array([0 for _ in range(900)] + [1 for _ in range(900)])\n",
    " \n",
    "# load all test reviews\n",
    "positive_docs = process_docs('dataset//txt_sentoken//pos', vocab, False)\n",
    "negative_docs = process_docs('dataset//txt_sentoken//neg', vocab, False)\n",
    "test_docs = negative_docs + positive_docs\n",
    "# sequence encode\n",
    "encoded_docs = tokenizer.texts_to_sequences(test_docs)\n",
    "# pad sequences\n",
    "Xtest = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "# define test labels\n",
    "ytest = array([0 for _ in range(100)] + [1 for _ in range(100)])\n",
    " \n",
    "# define vocabulary size (largest integer value)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    " \n",
    "# load embedding from file\n",
    "raw_embedding = load_embedding('embedding_word2vec.txt')\n",
    "# get vectors in the right order\n",
    "embedding_vectors = get_weight_matrix(raw_embedding, tokenizer.word_index)\n",
    "# create the embedding layer\n",
    "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_vectors], input_length=max_length, trainable=False)\n",
    " \n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())\n",
    "# compile network\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "model.fit(Xtrain, ytrain, epochs=10, verbose=2)\n",
    "# evaluate\n",
    "loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b70aa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the example shows that performance was not improved.\n",
    "# In fact, performance was a lot worse. The results show that the training dataset was learned successfully, \n",
    "# but evaluation on the test dataset was very poor, at just above 50% accuracy.\n",
    "\n",
    "# The cause of the poor test performance may be because of the chosen word2vec configuration or the chosen neural \n",
    "# network configuration.\n",
    "# The weights in the embedding layer can be used as a starting point for the network, and adapted during the training \n",
    "# of the network. We can do this by setting ‘trainable=True‘ (the default) in the creation of the embedding layer.\n",
    "\n",
    "# Repeating the experiment with this change shows slightly better results, but still poor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "04a552e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is possible to use pre-trained word vectors prepared on very large corpora of text data.\n",
    "# For example, both Google and Stanford provide pre-trained word vectors that you can download, trained with the efficient \n",
    "# word2vec and GloVe methods respectively. https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "# Let’s try to use pre-trained vectors in our model.\n",
    "# we downloaded glove.6B.zip (822 Megabyte download)\n",
    "# Unzipping the file, you will find pre-trained embeddings for various different dimensions. We will load the 100 \n",
    "# dimension version in the file ‘glove.6B.100d.txt‘\n",
    "\n",
    "# The Glove file does not contain a header file, so we do not need to skip the first line when loading the embedding \n",
    "# into memory. We updated the load_embedding() function.\n",
    "# It is possible that the loaded embedding does not contain all of the words in our chosen vocabulary. As such, when creating \n",
    "# the Embedding weight matrix, we need to skip words that do not have a corresponding vector in the loaded GloVe data. \n",
    "# Below is the updated, more defensive version of the get_weight_matrix() function.\n",
    "\n",
    "# Next, we can load the GloVe embedding and create the Embedding layer as before.\n",
    "# We will use the same model as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "0e729d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_14 (Embedding)    (None, 1244, 100)         1478100   \n",
      "                                                                 \n",
      " conv1d_13 (Conv1D)          (None, 1240, 128)         64128     \n",
      "                                                                 \n",
      " max_pooling1d_13 (MaxPoolin  (None, 620, 128)         0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " flatten_12 (Flatten)        (None, 79360)             0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 1)                 79361     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,621,589\n",
      "Trainable params: 143,489\n",
      "Non-trainable params: 1,478,100\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/6\n",
      "57/57 - 7s - loss: 0.7091 - accuracy: 0.5656 - 7s/epoch - 115ms/step\n",
      "Epoch 2/6\n",
      "57/57 - 6s - loss: 0.4492 - accuracy: 0.7967 - 6s/epoch - 113ms/step\n",
      "Epoch 3/6\n",
      "57/57 - 7s - loss: 0.2306 - accuracy: 0.9383 - 7s/epoch - 115ms/step\n",
      "Epoch 4/6\n",
      "57/57 - 6s - loss: 0.0955 - accuracy: 0.9956 - 6s/epoch - 106ms/step\n",
      "Epoch 5/6\n",
      "57/57 - 6s - loss: 0.0580 - accuracy: 0.9983 - 6s/epoch - 104ms/step\n",
      "Epoch 6/6\n",
      "57/57 - 6s - loss: 0.0260 - accuracy: 1.0000 - 6s/epoch - 108ms/step\n",
      "Test Accuracy: 70.999998\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "from os import listdir\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    " \n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r',encoding='utf8')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    " \n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc, vocab):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # filter out tokens not in vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens\n",
    " \n",
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab, is_trian):\n",
    "    documents = list()\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # skip any reviews in the test set\n",
    "        if is_trian and filename.startswith('cv9'):\n",
    "            continue\n",
    "        if not is_trian and not filename.startswith('cv9'):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load the doc\n",
    "        doc = load_doc(path)\n",
    "        # clean doc\n",
    "        tokens = clean_doc(doc, vocab)\n",
    "        # add to list\n",
    "        documents.append(tokens)\n",
    "    return documents\n",
    " \n",
    "# load embedding as a dict\n",
    "def load_embedding(filename):\n",
    "    # load embedding into memory, skip first line\n",
    "    file = open(filename,'r',encoding='utf8')\n",
    "    lines = file.readlines()\n",
    "    file.close()\n",
    "    # create a map of words to vectors\n",
    "    embedding = dict()\n",
    "    for line in lines:\n",
    "        parts = line.split()\n",
    "        # key is string word, value is numpy array for vector\n",
    "        embedding[parts[0]] = asarray(parts[1:], dtype='float32')\n",
    "    return embedding\n",
    " \n",
    "# create a weight matrix for the Embedding layer from a loaded embedding\n",
    "def get_weight_matrix(embedding, vocab):\n",
    "    # total vocabulary size plus 0 for unknown words\n",
    "    vocab_size = len(vocab) + 1\n",
    "    # define weight matrix dimensions with all 0\n",
    "    weight_matrix = zeros((vocab_size, 100))\n",
    "    # step vocab, store vectors using the Tokenizer's integer mapping\n",
    "    for word, i in vocab.items():\n",
    "        vector = embedding.get(word)\n",
    "        if vector is not None:\n",
    "            weight_matrix[i] = vector\n",
    "    return weight_matrix\n",
    " \n",
    "# load the vocabulary\n",
    "vocab_filename = 'vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)\n",
    " \n",
    "# load all training reviews\n",
    "positive_docs = process_docs('dataset//txt_sentoken//pos', vocab, True)\n",
    "negative_docs = process_docs('dataset//txt_sentoken//neg', vocab, True)\n",
    "train_docs = negative_docs + positive_docs\n",
    " \n",
    "# create the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "tokenizer.fit_on_texts(train_docs)\n",
    " \n",
    "# sequence encode\n",
    "encoded_docs = tokenizer.texts_to_sequences(train_docs)\n",
    "# pad sequences\n",
    "max_length = max([len(s.split()) for s in train_docs])\n",
    "Xtrain = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "# define training labels\n",
    "ytrain = array([0 for _ in range(900)] + [1 for _ in range(900)])\n",
    " \n",
    "# load all test reviews\n",
    "positive_docs = process_docs('dataset//txt_sentoken//pos', vocab, False)\n",
    "negative_docs = process_docs('dataset//txt_sentoken//neg', vocab, False)\n",
    "test_docs = negative_docs + positive_docs\n",
    "# sequence encode\n",
    "encoded_docs = tokenizer.texts_to_sequences(test_docs)\n",
    "# pad sequences\n",
    "Xtest = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "# define test labels\n",
    "ytest = array([0 for _ in range(100)] + [1 for _ in range(100)])\n",
    " \n",
    "# define vocabulary size (largest integer value)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    " \n",
    "# load embedding from file\n",
    "raw_embedding = load_embedding('GloVe//glove.6B.100d.txt')\n",
    "# get vectors in the right order\n",
    "embedding_vectors = get_weight_matrix(raw_embedding, tokenizer.word_index)\n",
    "# create the embedding layer\n",
    "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_vectors], input_length=max_length, trainable=False)\n",
    " \n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())\n",
    "# compile network\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "model.fit(Xtrain, ytrain, epochs=6, verbose=2)\n",
    "# evaluate\n",
    "loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44f442f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34743014",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
