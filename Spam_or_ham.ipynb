{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7587fe20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a7bb0c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# este código realiza una salida de una tabla con datos ya previamente procesados, quedándonos las partes que nos interesan. \n",
    "\n",
    "spam_or_ham = pd.read_csv(\"dataset//spam.csv\", encoding='latin-1')[[\"v1\", \"v2\"]]\n",
    "spam_or_ham.columns = [\"label\", \"text\"]\n",
    "spam_or_ham.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91e74151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     4825\n",
       "spam     747\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Con esta línea de código se pueden contar cuántas muestras tenemos de textos clasificados como spam o no.\n",
    "spam_or_ham[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c019bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora podemos definir una función que nos servirá para realizar el proceso de tokenización y \n",
    "# eliminación de las partes innecesarias de las muestras.\n",
    "\n",
    "import string\n",
    "punctuation = set(string.punctuation)\n",
    "def tokenize(sentence):\n",
    "    tokens = []\n",
    "    for token in sentence.split():\n",
    "        new_token = []\n",
    "        for character in token:\n",
    "            if character not in punctuation:\n",
    "                new_token.append(character.lower())\n",
    "        if new_token:\n",
    "            tokens.append(\"\".join(new_token))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3df2c88d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [go, until, jurong, point, crazy, available, o...\n",
       "1                        [ok, lar, joking, wif, u, oni]\n",
       "2     [free, entry, in, 2, a, wkly, comp, to, win, f...\n",
       "3     [u, dun, say, so, early, hor, u, c, already, t...\n",
       "4     [nah, i, dont, think, he, goes, to, usf, he, l...\n",
       "5     [freemsg, hey, there, darling, its, been, 3, w...\n",
       "6     [even, my, brother, is, not, like, to, speak, ...\n",
       "7     [as, per, your, request, melle, melle, oru, mi...\n",
       "8     [winner, as, a, valued, network, customer, you...\n",
       "9     [had, your, mobile, 11, months, or, more, u, r...\n",
       "10    [im, gonna, be, home, soon, and, i, dont, want...\n",
       "11    [six, chances, to, win, cash, from, 100, to, 2...\n",
       "12    [urgent, you, have, won, a, 1, week, free, mem...\n",
       "13    [ive, been, searching, for, the, right, words,...\n",
       "14           [i, have, a, date, on, sunday, with, will]\n",
       "15    [xxxmobilemovieclub, to, use, your, credit, cl...\n",
       "16                            [oh, kim, watching, here]\n",
       "17    [eh, u, remember, how, 2, spell, his, name, ye...\n",
       "18    [fine, if, thatåõs, the, way, u, feel, thatåõs...\n",
       "19    [england, v, macedonia, dont, miss, the, goals...\n",
       "20    [is, that, seriously, how, you, spell, his, name]\n",
       "21    [iû÷m, going, to, try, for, 2, months, ha, ha...\n",
       "22    [so, ì, pay, first, lar, then, when, is, da, s...\n",
       "23    [aft, i, finish, my, lunch, then, i, go, str, ...\n",
       "24    [ffffffffff, alright, no, way, i, can, meet, u...\n",
       "25    [just, forced, myself, to, eat, a, slice, im, ...\n",
       "26                  [lol, your, always, so, convincing]\n",
       "27    [did, you, catch, the, bus, are, you, frying, ...\n",
       "28    [im, back, amp, were, packing, the, car, now, ...\n",
       "29    [ahhh, work, i, vaguely, remember, that, what,...\n",
       "30    [wait, thats, still, not, all, that, clear, we...\n",
       "31    [yeah, he, got, in, at, 2, and, was, v, apolog...\n",
       "32                  [k, tell, me, anything, about, you]\n",
       "33    [for, fear, of, fainting, with, the, of, all, ...\n",
       "34    [thanks, for, your, subscription, to, ringtone...\n",
       "35    [yup, ok, i, go, home, look, at, the, timings,...\n",
       "36    [oops, ill, let, you, know, when, my, roommate...\n",
       "37                [i, see, the, letter, b, on, my, car]\n",
       "38                           [anything, lor, u, decide]\n",
       "39    [hello, hows, you, and, how, did, saturday, go...\n",
       "40    [pls, go, ahead, with, watts, i, just, wanted,...\n",
       "41    [did, i, forget, to, tell, you, i, want, you, ...\n",
       "42    [07732584351, rodger, burns, msg, we, tried, t...\n",
       "43                              [who, are, you, seeing]\n",
       "44    [great, i, hope, you, like, your, man, well, e...\n",
       "45                     [no, callsmessagesmissed, calls]\n",
       "46    [didnt, you, get, hep, b, immunisation, in, ni...\n",
       "47                  [fair, enough, anything, going, on]\n",
       "48    [yeah, hopefully, if, tyler, cant, do, it, i, ...\n",
       "49    [u, dont, know, how, stubborn, i, am, i, didnt...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Es un algoritmo básico que simplemente separa las frases en arrays de palabras y elimina signos de puntuación. \n",
    "# Lo normal aquí sería eliminar más elementos innecesarios pero al menos se capta la idea detrás de esta etapa \n",
    "# del proceso de machine learning. \n",
    "# Aplicamos el algoritmo de tokenización sobre la muestra con el siguiente código\n",
    "spam_or_ham.head(50)[\"text\"].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ef781f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora entramos en la parte más compleja de esta práctica, en la que usamos la librería scikit-learn a fin de realizar \n",
    "# el trabajo pesado del proceso de aprendizaje y pruebas. \n",
    "# Comenzamos con la configuración de los parámetros necesarios para  \n",
    "# aprender sobre la base de las muestras obtenidas en el archivo csv\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "demo_vectorizer = CountVectorizer(\n",
    "    tokenizer = tokenize,\n",
    "    binary = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fadf7bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples: 4179, testing examples 1393\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2024     ham\n",
       "2531     ham\n",
       "4309    spam\n",
       "4639     ham\n",
       "4634     ham\n",
       "3273     ham\n",
       "3328     ham\n",
       "118      ham\n",
       "1570     ham\n",
       "4721     ham\n",
       "Name: label, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Esta es la configuración que necesitaremos para obtener la tabla de datos ya procesados sobre las muestras, que será el\n",
    "# motor que usaremos para interpretar todos los futuros mensajes y catalogarlos como spam o no. En resumen, estamos diciendo \n",
    "# qué función tiene que usar para la tokenización y que ésta debe ser binaria, es decir, no importa el número de veces que \n",
    "# aparece una palabra, simplemente mirará si aparece o no. Con el siguiente código separamos los datos entre entrenamiento \n",
    "# y pruebas.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_text, test_text, train_labels, test_labels = train_test_split(spam_or_ham[\"text\"], spam_or_ham[\"label\"], stratify=spam_or_ham[\"label\"])\n",
    "print(f\"Training examples: {len(train_text)}, testing examples {len(test_text)}\")\n",
    "\n",
    "train_labels.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5690eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Después de haber hecho esta separación podemos pasar al siguiente paso, que consiste en crear un nuevo vectorizador, \n",
    "# desde cero, en el que solamente vamos a usar los datos de entrenamiento, no los datos de pruebas.\n",
    "\n",
    "real_vectorizer = CountVectorizer(tokenizer = tokenize, binary=True)\n",
    "train_X = real_vectorizer.fit_transform(train_text)\n",
    "test_X = real_vectorizer.transform(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89592bf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ahora volvemos a usar nuevas utilidades de la librería scikit-learn para seguir con nuestro trabajo de \n",
    "# preparación del Machine Learning. En esta etapa creamos el nuevo clasificador y usamos el método fit() \n",
    "# para procesar los datos, lo que prepara al clasificador para usarlo más adelante. De nuevo, usamos los datos \n",
    "# de entrenamiento para prepararlo, no los de prueba.\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "classifier = LinearSVC()\n",
    "classifier.fit(train_X, train_labels)\n",
    "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
    "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
    "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
    "          verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6727a0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.9899%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "En este punto nuestro clasificador está listo para trabajar con él, pero antes de ello podemos realizar una operación muy \n",
    "interesante que se basa en predecir la precisión de las clasificaciones que conseguirá. Se usa el método predict() que \n",
    "permite realizar finalmente las clasificaciones.\n",
    "\n",
    "Para calcular la precisión de nuestro sistema de Machine Learning usamos los datos de prueba, los cuales sabemos los \n",
    "resultados que deben entregar. Comparando las predicciones con los datos reales que tenían los datos de prueba podemos \n",
    "calcular la precisión que tendrá. En este bloque de código realizamos todos esos pasos para el cálculo de la precisión, \n",
    "en el que nos ayudamos de una función de scikit-learn llamada accuracy_score() que nos sirve para calcular la puntuación \n",
    "de manera sencilla\n",
    "\"\"\"\n",
    "from sklearn.metrics import accuracy_score\n",
    "predicciones = classifier.predict(test_X)\n",
    "accuracy = accuracy_score(test_labels, predicciones)\n",
    "print(f\"Accuracy: {accuracy:.4%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29287124",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Clasificar textos nuevos y comprobar el sistema Machine Learning\n",
    "Por fin podemos llegar al final de nuestra práctica, que consiste en crear una serie de frases para comprobar si son spam o no. \n",
    "Estas frases las podemos insertar en un array:\n",
    "\"\"\"\n",
    "frases = [\n",
    "  'Are you looking to redesign your website with new modern look and feel?',\n",
    "  'Please send me a confirmation of complete and permanent erasure of the personal data',\n",
    "  'You have been selected to win a FREE suscription to our service',\n",
    "  'We’re contacting you because the webhook endpoint associated with your account in test mode has been failing'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7521c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A continuación las pasamos por nuestro algoritmo de transformación y vectorización, para finalmente recibir las \n",
    "predicciones de clasificación.\n",
    "\"\"\"\n",
    "frases_X = real_vectorizer.transform(frases)\n",
    "predicciones = classifier.predict(frases_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51669044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam  - Are you looking to redesign your website with new modern look and feel?\n",
      "ham   - Please send me a confirmation of complete and permanent erasure of the personal data\n",
      "spam  - You have been selected to win a FREE suscription to our service\n",
      "ham   - We’re contacting you because the webhook endpoint associated with your account in test mode has been failing\n"
     ]
    }
   ],
   "source": [
    "# Con este último bloque podemos recorrer las predicciones y mostrar lo que nuestro sistema ha sido capaz de clasificar.\n",
    "\n",
    "for text, label in zip(frases, predicciones):\n",
    "    print(f\"{label:5} - {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78c6401",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
